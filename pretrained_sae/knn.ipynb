{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc6e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "dat shape (neurons √ó trials): (39209, 5900)\n",
      "\n",
      "‚è¨ Checking/Downloading: zer0int/CLIP-SAE-ViT-L-14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82892603afd34029b5f47bc3b7e57737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Files present.\n",
      "\n",
      "üîç Scanning safetensors for encoder-like matrices...\n",
      "  model.safetensors: text_model.embeddings.token_embedding.weight shape=(49408, 768)\n",
      "  model.safetensors: text_model.encoder.layers.0.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.0.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.1.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.1.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.10.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.10.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.11.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.11.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.2.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.2.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.3.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.3.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.4.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.4.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.5.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.5.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.6.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.6.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.7.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.7.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.8.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.8.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.9.mlp.fc1.weight shape=(3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.9.mlp.fc2.weight shape=(768, 3072)\n",
      "  model.safetensors: vision_model.encoder.layers.0.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.mlp.fc1.weight shape=(4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.mlp.fc2.weight shape=(1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.embeddings.token_embedding.weight shape=(49408, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.0.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.0.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.1.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.1.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.10.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.10.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.11.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.11.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.2.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.2.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.3.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.3.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.4.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.4.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.5.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.5.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.6.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.6.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.7.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.7.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.8.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.8.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.9.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.9.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.embeddings.token_embedding.weight shape=(49408, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.0.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.0.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.1.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.1.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.10.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.10.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.11.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.11.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.2.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.2.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.3.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.3.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.4.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.4.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.5.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.5.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.6.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.6.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.7.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.7.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.8.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.8.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.9.mlp.fc1.weight shape=(3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.9.mlp.fc2.weight shape=(768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.mlp.fc1.weight shape=(4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.mlp.fc2.weight shape=(1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.k_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.out_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.q_proj.weight shape=(1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.v_proj.weight shape=(1024, 1024)\n",
      "‚Üí Selected text_model.embeddings.token_embedding.weight from model.safetensors with shape torch.Size([49408, 768])\n",
      "Loaded encoder weight W: (49408, 768), bias found: True\n",
      "‚úì Matched encoder to extractor='image_proj_768' (in_dim=768, out_dim=49408, transpose=False)\n",
      "Final encoder weight: (49408, 768), bias: (49408,)\n",
      "\n",
      "Extracting features (image_proj_768)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:08<00:00, 13.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CLIP shape: (118, 1024) | SAE latent shape: (118, 49408)\n",
      "\n",
      "Running 5-fold CV (KNN latent mapping, K=3) on 10 neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neurons: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:26<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved ‚Üí clip_sae_vitl14_knn_results.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# =====================================================================\n",
    "# Allen Mouse Neurons ‚Äî CLIP-SAE-ViT-L-14  ‚Üí Neuron Mapping via KNN\n",
    "# =====================================================================\n",
    "\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------------\n",
    "# Config / constants\n",
    "# -----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng(42)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "N_IMAGES = 118\n",
    "N_TRIALS = 50\n",
    "DATA_PATH = \"/home/maria/LuckyMouse/pixel_transformer_neuro/data/processed/hybrid_neural_responses.npy\"\n",
    "IMG_DIR = Path(\"/home/maria/MITNeuralComputation/vit_embeddings/images\")\n",
    "SAE_REPO = \"zer0int/CLIP-SAE-ViT-L-14\"\n",
    "LOCAL_SAE_DIR = \"./clip_sae_vitl14_weights\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Brier loss\n",
    "# -----------------------------\n",
    "def compute_brier(y_true, y_pred, img_ids):\n",
    "    df = pd.DataFrame({\"img\": img_ids, \"y\": y_true, \"p\": y_pred})\n",
    "    agg = df.groupby(\"img\").mean()\n",
    "    return float(np.mean((agg[\"p\"] - agg[\"y\"]) ** 2))\n",
    "\n",
    "# -----------------------------\n",
    "# Load neural data\n",
    "# -----------------------------\n",
    "dat = np.load(DATA_PATH)\n",
    "print(\"dat shape (neurons √ó trials):\", dat.shape)\n",
    "Y_binary = (dat > 0).astype(int)\n",
    "n_neurons, n_samples = dat.shape\n",
    "assert n_samples == N_IMAGES * N_TRIALS\n",
    "img_ids_full = np.repeat(np.arange(N_IMAGES), N_TRIALS)\n",
    "\n",
    "# -----------------------------\n",
    "# Download SAE repo if needed\n",
    "# -----------------------------\n",
    "print(\"\\n‚è¨ Checking/Downloading:\", SAE_REPO)\n",
    "snapshot_download(repo_id=SAE_REPO, local_dir=LOCAL_SAE_DIR)\n",
    "print(\"‚úì Files present.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load CLIP backbone + processor\n",
    "# -----------------------------\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n",
    "clip_model.eval()\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "vision_model: CLIPVisionModel = clip_model.vision_model\n",
    "\n",
    "# -----------------------------\n",
    "# Find SAE encoder matrix\n",
    "# -----------------------------\n",
    "print(\"\\nüîç Scanning safetensors for encoder-like matrices...\")\n",
    "candidates = []\n",
    "for f in Path(LOCAL_SAE_DIR).glob(\"*.safetensors\"):\n",
    "    with safe_open(f, framework=\"pt\", device=\"cpu\") as sf:\n",
    "        for k in sf.keys():\n",
    "            t = sf.get_tensor(k)\n",
    "            if t.ndim == 2:\n",
    "                candidates.append((f, k, t.shape))\n",
    "                if t.shape[0] * t.shape[1] > 1_000_000:\n",
    "                    print(f\"  {f.name}: {k} shape={tuple(t.shape)}\")\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\"No 2D tensors found in any .safetensors file.\")\n",
    "sae_path, enc_key, enc_shape = max(candidates, key=lambda x: x[2][0] * x[2][1])\n",
    "print(f\"‚Üí Selected {enc_key} from {sae_path.name} with shape {enc_shape}\")\n",
    "\n",
    "with safe_open(sae_path, framework=\"pt\", device=DEVICE) as f:\n",
    "    W = f.get_tensor(enc_key)\n",
    "    bias = None\n",
    "    for k in f.keys():\n",
    "        if \"bias\" in k.lower():\n",
    "            b = f.get_tensor(k)\n",
    "            if b.ndim == 1 and (b.shape[0] == W.shape[0] or b.shape[0] == W.shape[1]):\n",
    "                bias = b\n",
    "                bias_key = k\n",
    "                break\n",
    "if bias is None:\n",
    "    bias = None\n",
    "print(f\"Loaded encoder weight W: {tuple(W.shape)}, bias found: {bias is not None}\")\n",
    "\n",
    "# -----------------------------\n",
    "# CLIP feature extractors\n",
    "# -----------------------------\n",
    "def feat_pooled_1024(inputs):\n",
    "    outs = vision_model(**inputs)\n",
    "    return outs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def feat_image_proj_768(inputs):\n",
    "    return clip_model.get_image_features(**inputs)\n",
    "\n",
    "def feat_flat_resid_with_cls(inputs):\n",
    "    outs = vision_model(**inputs)\n",
    "    x = outs.last_hidden_state\n",
    "    return x.flatten(start_dim=1)\n",
    "\n",
    "def feat_flat_resid_no_cls(inputs):\n",
    "    outs = vision_model(**inputs)\n",
    "    x = outs.last_hidden_state[:, 1:, :]\n",
    "    return x.flatten(start_dim=1)\n",
    "\n",
    "extractors = [\n",
    "    (\"pooled_1024\",        feat_pooled_1024,        1024),\n",
    "    (\"image_proj_768\",     feat_image_proj_768,      768),\n",
    "    (\"flat_resid_withCLS\", feat_flat_resid_with_cls, 257 * 1024),\n",
    "    (\"flat_resid_noCLS\",   feat_flat_resid_no_cls,   256 * 1024),\n",
    "]\n",
    "\n",
    "def pick_mapping(W, bias):\n",
    "    A, B = W.shape\n",
    "    for name, fn, dim in extractors:\n",
    "        if dim == B:\n",
    "            out_dim, in_dim, use_T, extractor = A, B, False, (name, fn)\n",
    "            b_vec = bias if (bias is not None and bias.shape[0] == A) else torch.zeros(A, device=DEVICE)\n",
    "            return out_dim, in_dim, use_T, extractor, b_vec\n",
    "        if dim == A:\n",
    "            out_dim, in_dim, use_T, extractor = B, A, True, (name, fn)\n",
    "            b_vec = bias if (bias is not None and bias.shape[0] == B) else torch.zeros(B, device=DEVICE)\n",
    "            return out_dim, in_dim, use_T, extractor, b_vec\n",
    "    return None\n",
    "\n",
    "mapping = pick_mapping(W, bias)\n",
    "if mapping is None:\n",
    "    raise RuntimeError(\"Could not match encoder to CLIP features.\")\n",
    "\n",
    "OUT_DIM, IN_DIM, USE_T, (EXTRACTOR_NAME, EXTRACTOR_FN), B_VEC = mapping\n",
    "print(f\"‚úì Matched encoder to extractor='{EXTRACTOR_NAME}' \"\n",
    "      f\"(in_dim={IN_DIM}, out_dim={OUT_DIM}, transpose={USE_T})\")\n",
    "\n",
    "W_use = W.T if USE_T else W\n",
    "if B_VEC.shape[0] != OUT_DIM:\n",
    "    B_VEC = torch.zeros(OUT_DIM, device=DEVICE)\n",
    "print(f\"Final encoder weight: {tuple(W_use.shape)}, bias: {tuple(B_VEC.shape)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# SAE encoder module\n",
    "# -----------------------------\n",
    "class SAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, W_init, b_init):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_dim, out_dim, bias=True)\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.copy_(W_init)\n",
    "            self.linear.bias.copy_(b_init)\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.linear(x))\n",
    "\n",
    "sae = SAEEncoder(IN_DIM, OUT_DIM, W_use.to(DEVICE), B_VEC.to(DEVICE)).to(DEVICE)\n",
    "sae.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Extract CLIP + SAE features\n",
    "# -----------------------------\n",
    "img_paths = sorted(IMG_DIR.glob(\"scene_*.png\"))\n",
    "assert len(img_paths) == N_IMAGES\n",
    "print(f\"\\nExtracting features ({EXTRACTOR_NAME})...\")\n",
    "\n",
    "clip_feats_1024, sae_feats = [], []\n",
    "for p in tqdm(img_paths, desc=\"Feature extraction\"):\n",
    "    image = Image.open(p).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        x_in = EXTRACTOR_FN(inputs)\n",
    "        z = sae(x_in)\n",
    "        x_bl = vision_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    clip_feats_1024.append(x_bl.squeeze().cpu().numpy())\n",
    "    sae_feats.append(z.squeeze().cpu().numpy())\n",
    "\n",
    "X_clip = np.stack(clip_feats_1024)\n",
    "Z_sae = np.stack(sae_feats)\n",
    "print(f\"Baseline CLIP shape: {X_clip.shape} | SAE latent shape: {Z_sae.shape}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare trial-wise data\n",
    "# -----------------------------\n",
    "X_clip_full = np.repeat(X_clip, N_TRIALS, axis=0)\n",
    "Z_sae_full  = np.repeat(Z_sae,  N_TRIALS, axis=0)\n",
    "scaler_clip = StandardScaler().fit(X_clip_full)\n",
    "scaler_sae  = StandardScaler().fit(Z_sae_full)\n",
    "X_clip_full = scaler_clip.transform(X_clip_full)\n",
    "Z_sae_full  = scaler_sae.transform(Z_sae_full)\n",
    "\n",
    "# -----------------------------\n",
    "# 5-Fold CV ‚Äî KNN latent mapping\n",
    "# -----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds = list(kf.split(np.arange(N_IMAGES)))\n",
    "\n",
    "num_neurons_to_sample = min(10, n_neurons)\n",
    "sampled_neurons = rng.choice(np.arange(n_neurons), size=num_neurons_to_sample, replace=False)\n",
    "K_NEIGHBORS = 3\n",
    "print(f\"\\nRunning 5-fold CV (KNN latent mapping, K={K_NEIGHBORS}) on {num_neurons_to_sample} neurons...\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for nid in tqdm(sampled_neurons, desc=\"Neurons\"):\n",
    "    y_all = Y_binary[nid]\n",
    "    b_sae_folds = []\n",
    "\n",
    "    for train_imgs, test_imgs in folds:\n",
    "        train_mask = np.isin(img_ids_full, train_imgs)\n",
    "        test_mask  = np.isin(img_ids_full, test_imgs)\n",
    "\n",
    "        Z_train,  Z_test  = Z_sae_full[train_mask],  Z_sae_full[test_mask]\n",
    "        y_train,  y_test  = y_all[train_mask],       y_all[test_mask]\n",
    "        test_img_ids      = img_ids_full[test_mask]\n",
    "\n",
    "        # collapse trials ‚Üí per-image means\n",
    "        y_train_mean = np.array([y_train[img_ids_full[train_mask] == i].mean() for i in train_imgs])\n",
    "        y_test_mean  = np.array([y_test[img_ids_full[test_mask] == i].mean() for i in test_imgs])\n",
    "        Z_train_mean = np.array([Z_train[img_ids_full[train_mask] == i].mean(axis=0) for i in train_imgs])\n",
    "        Z_test_mean  = np.array([Z_test[img_ids_full[test_mask] == i].mean(axis=0) for i in test_imgs])\n",
    "\n",
    "        # transpose: latents as samples\n",
    "        Z_train_T = Z_train_mean.T\n",
    "        # remove inactive latents\n",
    "        active_mask = Z_train_T.std(axis=1) > 1e-6\n",
    "        Z_train_T = Z_train_T[active_mask]\n",
    "        if Z_train_T.shape[0] == 0:\n",
    "            continue\n",
    "        Z_train_T = StandardScaler(with_mean=True, with_std=True).fit_transform(Z_train_T)\n",
    "\n",
    "        # KNN search\n",
    "        nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric=\"cosine\")\n",
    "        nbrs.fit(Z_train_T)\n",
    "        distances, indices = nbrs.kneighbors(y_train_mean.reshape(1, -1))\n",
    "        best_idxs, weights = indices[0], 1 - distances[0]\n",
    "        weights = weights / np.sum(weights)\n",
    "\n",
    "        # test side, same active latents\n",
    "        Z_test_T = Z_test_mean.T[active_mask]\n",
    "        Z_test_T = StandardScaler(with_mean=True, with_std=True).fit_transform(Z_test_T)\n",
    "\n",
    "        # predict via weighted combo of chosen latents\n",
    "        z_train_pred = np.average(Z_train_T[best_idxs, :], axis=0, weights=weights)\n",
    "        z_test_pred  = np.average(Z_test_T[best_idxs, :], axis=0, weights=weights)\n",
    "\n",
    "        # linear calibration on train\n",
    "        var_z = np.var(z_train_pred)\n",
    "        if var_z < 1e-8:\n",
    "            a, b = 0.0, y_train_mean.mean()\n",
    "        else:\n",
    "            a = np.cov(y_train_mean, z_train_pred)[0, 1] / var_z\n",
    "            b = y_train_mean.mean() - a * z_train_pred.mean()\n",
    "\n",
    "        y_pred_test = np.clip(a * z_test_pred + b, 0, 1)\n",
    "\n",
    "        brier = float(np.mean((y_pred_test - y_test_mean) ** 2))\n",
    "        b_sae_folds.append(brier)\n",
    "\n",
    "    if len(b_sae_folds) == 0:\n",
    "        continue\n",
    "\n",
    "    records.append({\n",
    "        \"neuron_idx\": int(nid),\n",
    "        \"Brier_SAE_mean\":  float(np.nanmean(b_sae_folds)),\n",
    "        \"Brier_SAE_std\":   float(np.nanstd(b_sae_folds)),\n",
    "        \"K\": K_NEIGHBORS,\n",
    "        \"sae_file\": sae_path.name,\n",
    "        \"enc_key\": enc_key,\n",
    "        \"extractor\": EXTRACTOR_NAME,\n",
    "        \"in_dim\": int(IN_DIM),\n",
    "        \"out_dim\": int(OUT_DIM),\n",
    "        \"transpose_used\": bool(USE_T),\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(records)\n",
    "df_cv.to_csv(\"clip_sae_vitl14_knn_results.csv\", index=False)\n",
    "print(\"\\n‚úÖ Saved ‚Üí clip_sae_vitl14_knn_results.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# Visualization\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.hist(df_cv[\"Brier_SAE_mean\"], bins=20, color=\"royalblue\", alpha=0.7)\n",
    "plt.xlabel(\"Brier loss (test)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"KNN-based SAE‚ÜíNeuron mapping ‚Äî 5-fold CV\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"clip_sae_vitl14_knn_hist.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680dc365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron_idx</th>\n",
       "      <th>Brier_SAE_mean</th>\n",
       "      <th>Brier_SAE_std</th>\n",
       "      <th>K</th>\n",
       "      <th>sae_file</th>\n",
       "      <th>enc_key</th>\n",
       "      <th>extractor</th>\n",
       "      <th>in_dim</th>\n",
       "      <th>out_dim</th>\n",
       "      <th>transpose_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3369</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30339</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3498</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25660</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17205</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16975</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27341</td>\n",
       "      <td>0.007175</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3692</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7899</td>\n",
       "      <td>0.006489</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33661</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>image_proj_768</td>\n",
       "      <td>768</td>\n",
       "      <td>49408</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neuron_idx  Brier_SAE_mean  Brier_SAE_std  K           sae_file  \\\n",
       "0        3369        0.003015       0.002940  3  model.safetensors   \n",
       "1       30339        0.001577       0.000654  3  model.safetensors   \n",
       "2        3498        0.000819       0.000298  3  model.safetensors   \n",
       "3       25660        0.000917       0.000405  3  model.safetensors   \n",
       "4       17205        0.000532       0.000140  3  model.safetensors   \n",
       "5       16975        0.001270       0.000610  3  model.safetensors   \n",
       "6       27341        0.007175       0.006245  3  model.safetensors   \n",
       "7        3692        0.001269       0.001203  3  model.safetensors   \n",
       "8        7899        0.006489       0.004669  3  model.safetensors   \n",
       "9       33661        0.002144       0.001096  3  model.safetensors   \n",
       "\n",
       "                                        enc_key       extractor  in_dim  \\\n",
       "0  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "1  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "2  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "3  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "4  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "5  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "6  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "7  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "8  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "9  text_model.embeddings.token_embedding.weight  image_proj_768     768   \n",
       "\n",
       "   out_dim  transpose_used  \n",
       "0    49408           False  \n",
       "1    49408           False  \n",
       "2    49408           False  \n",
       "3    49408           False  \n",
       "4    49408           False  \n",
       "5    49408           False  \n",
       "6    49408           False  \n",
       "7    49408           False  \n",
       "8    49408           False  \n",
       "9    49408           False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05f85908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean prob of spiking: 0.047591611403642625\n",
      "Typical range: 0.007288135593220339 0.42406779661016947\n",
      "Brier of constant baseline ‚âà 0.001258400568767347\n"
     ]
    }
   ],
   "source": [
    "# how sparse are the empirical probabilities?\n",
    "y_means = Y_binary.mean(axis=1)           # mean firing prob per neuron\n",
    "print(\"Mean prob of spiking:\", y_means.mean())\n",
    "print(\"Typical range:\", y_means.min(), y_means.max())\n",
    "\n",
    "# compare to naive baseline\n",
    "naive_brier = np.mean((y_means.mean() - y_means) ** 2)\n",
    "print(\"Brier of constant baseline ‚âà\", naive_brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5470e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "dat shape (neurons √ó trials): (39209, 5900)\n",
      "Mean prob of spiking: 0.0476\n",
      "Typical range: 0.0073 ‚Äì 0.4241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92ae6eeb902414eb805de7609e0fcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Scanning safetensors for encoder-like matrices...\n",
      "  model.safetensors: text_model.embeddings.token_embedding.weight (49408, 768)\n",
      "  model.safetensors: text_model.encoder.layers.0.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.0.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.1.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.1.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.10.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.10.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.11.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.11.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.2.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.2.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.3.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.3.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.4.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.4.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.5.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.5.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.6.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.6.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.7.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.7.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.8.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.8.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: text_model.encoder.layers.9.mlp.fc1.weight (3072, 768)\n",
      "  model.safetensors: text_model.encoder.layers.9.mlp.fc2.weight (768, 3072)\n",
      "  model.safetensors: vision_model.encoder.layers.0.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.0.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.1.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.10.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.11.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.12.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.13.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.14.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.15.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.16.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.17.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.18.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.19.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.2.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.20.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.21.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.22.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.23.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.3.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.4.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.5.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.6.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.7.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.8.self_attn.v_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.mlp.fc1.weight (4096, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.mlp.fc2.weight (1024, 4096)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.k_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.out_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.q_proj.weight (1024, 1024)\n",
      "  model.safetensors: vision_model.encoder.layers.9.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.embeddings.token_embedding.weight (49408, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.0.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.0.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.1.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.1.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.10.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.10.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.11.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.11.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.2.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.2.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.3.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.3.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.4.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.4.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.5.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.5.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.6.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.6.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.7.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.7.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.8.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.8.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.9.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-TE-only.safetensors: text_model.encoder.layers.9.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.embeddings.token_embedding.weight (49408, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.0.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.0.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.1.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.1.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.10.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.10.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.11.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.11.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.2.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.2.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.3.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.3.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.4.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.4.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.5.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.5.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.6.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.6.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.7.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.7.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.8.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.8.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.9.mlp.fc1.weight (3072, 768)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: text_model.encoder.layers.9.mlp.fc2.weight (768, 3072)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.0.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.1.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.10.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.11.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.12.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.13.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.14.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.15.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.16.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.17.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.18.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.19.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.2.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.20.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.21.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.22.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.23.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.3.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.4.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.5.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.6.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.7.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.8.self_attn.v_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.mlp.fc1.weight (4096, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.mlp.fc2.weight (1024, 4096)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.k_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.out_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.q_proj.weight (1024, 1024)\n",
      "  ViT-L-14-GmP-SAE-FULL-model.safetensors: vision_model.encoder.layers.9.self_attn.v_proj.weight (1024, 1024)\n",
      "‚Üí Selected model.safetensors, key=text_model.embeddings.token_embedding.weight, shape=torch.Size([49408, 768])\n",
      "‚ö†Ô∏è Bias mismatch ‚Äî creating zero bias of length 49408\n",
      "‚úì SAE Linear(in_dim=768, out_dim=49408) ready\n",
      "\n",
      "Extracting SAE features for 118 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extraction: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [00:04<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE latent shape: (118, 49408)\n",
      "\n",
      "Running filtered KNN (K=3) on 10 neurons...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neurons: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved ‚Üí clip_sae_vitl14_knn_filtered_results.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAHqCAYAAADyPMGQAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUdhJREFUeJzt3Xl4VOX9///XTJbJPiGYEKIxYZFFXMBQIqiABmQTxA1BKosUtNWiwudjoW4gRariiq0W/cqiUGi1bkihbC4foIARRCCyKEQFQsBAFkJCkrl/f8wvA2MWTkKSyfJ8XFeuds76vt9O4MU595yxGWOMAAAAcE52XxcAAADQUBCcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABYRnNAoHDhwQDabTfPnz/csmzZtmmw2m++KqqKGVi/QlHz66aey2Wz69NNPfV0KfIzghAZh/vz5stls5f5MmTLF8nGefvppffDBB7VXaB0YM2aMwsLCyizfvn27LrjgAiUmJurAgQOSpN69e8tms2nw4MFlti8Nm7Nnz/YsK/3LwWazKTU11fK569LHH3+sXr16KSYmRiEhIWrdurWGDRumFStWlLt9SUmJ4uLiZLPZ9O9//7vcbUpDa0U/GRkZ51VzQ+grAGv8fV0AUBVPPfWUWrVq5bXssssuU0JCgk6dOqWAgIBK93/66ad1++23a+jQobVYZd3bsWOHUlJSFBoaqnXr1ikxMdFr/bJly5SamqqkpCTLx5w2bZo+/vjjGq70/MyePVv/+7//q169emnq1KkKCQnRvn37tHr1ai1ZskT9+/cvs8/atWt1+PBhJSYmatGiRRowYECFx3/ttdfKDTCRkZE1Nob62FecW8+ePXXq1CkFBgb6uhT4GMEJDcqAAQPUtWvXctcFBQXVcTVuBQUFCgwMlN3umwu4O3fu1A033KDg4GCtW7euTLC8+OKLlZubq+nTp+ujjz6ydMzOnTtr2bJl+uqrr3TVVVfVRtlVVlxcrBkzZqhv3776z3/+U2Z9ZmZmufu98847uuqqqzR69Gj98Y9/1MmTJxUaGlrutrfffrsuuOCCGq37bPWtr8XFxXK5XIQBC+x2u8/+jEH9wq06NArlzXH6JZvNppMnT2rBggWe2yZjxozxrD948KDuuecetWjRQg6HQ506ddJbb73ldYzSWy5LlizRY489pgsvvFAhISHKycmRJG3atEn9+/eX0+lUSEiIevXqpfXr15ep5f/+7//0q1/9SkFBQWrTpo3+9re/VWvcaWlpSklJkcPh0Lp169S6desy24SHh+vhhx/Wxx9/rK+++srScX//+9+rWbNmmjZtWrXqqg3Hjh1TTk6OrrnmmnLXx8TElFl26tQpvf/++xo+fLiGDRumU6dO6cMPP6ztUitU1b7++9//1nXXXafQ0FCFh4dr0KBB2rlzp9c2vXv3Vu/evcvsO2bMGK8rj2ffmn3ppZfUpk0bORwO7dq1S5L7ylzpuSIjI3XzzTcrLS3N65iltzT37dunMWPGKDIyUk6nU2PHjlV+fv45x9O7d29ddtll2r59u3r16qWQkBC1bdtW7777riTps88+U3JysoKDg9W+fXutXr3aa//09HT97ne/U/v27RUcHKzmzZvrjjvu8NyaLlV6a//zzz/Xvffeq+bNmysiIkKjRo3S8ePHvbZNTEzUTTfdpP/85z/q3LmzgoKCdOmll+pf//qX13blzXEqHc+uXbt0/fXXKyQkRBdeeKGeffbZMmNPT0/XkCFDFBoaqpiYGD388MNauXIl86YaIIITGpTs7GwdO3bM68eqt99+Ww6HQ9ddd53efvttvf3227r33nslSUeOHNHVV1+t1atX64EHHtDLL7+stm3baty4cXrppZfKHGvGjBn65JNP9D//8z96+umnFRgYqLVr16pnz57KycnRk08+qaefflonTpzQDTfcoM2bN3v2/eabb3TjjTcqMzNT06ZN09ixY/Xkk0/q/fffr1Ivdu/erRtuuEH+/v5at26d2rRpU+G2Dz74YJX+wo6IiKhy2KptMTExCg4O1scff6ysrCxL+3z00UfKy8vT8OHDFRsbq969e2vRokUVbp+VlVXm/XXixAnLNS5fvly/+c1v5HK5yl1flb6+/fbbGjRokMLCwvTMM8/o8ccf165du3TttdeWCQpVMW/ePM2ZM0cTJkzQ888/r6ioKK1evVr9+vXzvCcnTZqkDRs26Jprrin3XMOGDVNubq5mzZqlYcOGaf78+Zo+fbql8x8/flw33XSTkpOT9eyzz8rhcGj48OFaunSphg8froEDB+rPf/6zTp48qdtvv125ubmefbds2aINGzZo+PDheuWVV3TfffdpzZo16t27d7nB7YEHHlBaWpqmTZumUaNGadGiRRo6dKiMMV7b7d27V3feeacGDBigWbNmyd/fX3fccYdWrVplaTz9+/fXlVdeqeeff14dOnTQH/7wB6/5dCdPntQNN9yg1atXa+LEiXr00Ue1YcMG/eEPf7DUM9QzBmgA5s2bZySV+2OMMfv37zeSzLx58zz7PPnkk+aXb/HQ0FAzevToMscfN26cadmypTl27JjX8uHDhxun02ny8/ONMcasW7fOSDKtW7f2LDPGGJfLZS655BLTr18/43K5PMvz8/NNq1atTN++fT3Lhg4daoKCgkx6erpn2a5du4yfn1+ZesszevRoExAQYFq2bGni4uLMnj17Kty2V69eplOnTsYYY6ZPn24kmdTUVGPMmZ4999xznu1Lx/fPf/7TnDhxwjRr1swMGTLE69yhoaHnrLG2PPHEE0aSCQ0NNQMGDDAzZ870jKc8N910k7nmmms8r+fOnWv8/f1NZmam13al75Xyftq3b2+5vqVLlxo/Pz8zbtw4r/dBVfuam5trIiMjzfjx472On5GRYZxOp9fyXr16mV69epWpZfTo0SYhIcHzuvS/d0RERJnxd+7c2cTExJiff/7Zs+zrr782drvdjBo1qkyf7rnnHq/9b7nlFtO8efNzdMddqySzePFiz7Jvv/3WSDJ2u93897//9SxfuXJlmd/ps3/nSm3cuNFIMgsXLvQsK/3zIikpyZw+fdqz/NlnnzWSzIcffuhZlpCQYCSZ9957z7MsOzvbtGzZ0nTp0sWzrPS/4bp168qM5+xzFxYWmtjYWHPbbbd5lj3//PNGkvnggw88y06dOmU6dOhQ5pio/7jihAblL3/5i1atWuX1c76MMXrvvfc0ePBgGWO8rjb069dP2dnZZa4OjB49WsHBwZ7X27Zt0969e3XXXXfp559/9ux/8uRJpaSk6PPPP5fL5VJJSYlWrlypoUOH6uKLL/bs37FjR/Xr189yzSUlJTp27JiioqIsz8kpvepk9cqA0+nUQw89pI8++khbt261XFttmj59uhYvXqwuXbpo5cqVevTRR5WUlKSrrrqqzG2ln3/+WStXrtSIESM8y2677TbZbDb94x//KPf47733Xpn317x58zzrT58+rYKCggp/hgwZor/97W966623dP/995d7Dit9XbVqlU6cOKERI0Z4vR/9/PyUnJysdevWVbV1Xj2Ijo72vD58+LC2bdumMWPGKCoqyrP8iiuuUN++fbV8+fIyx7jvvvu8Xl933XX6+eefPbesKxMWFqbhw4d7Xrdv316RkZHq2LGjkpOTPctL///333/vWXb271xRUZF+/vlntW3bVpGRkeVewZswYYLXB0Z++9vfyt/fv8yY4uLidMstt3hel97W27p16zk/URkWFqZf//rXnteBgYHq1q2bV90rVqzQhRdeqCFDhniWBQUFafz48ZUeG/UTk8PRoHTr1q3CyeHVdfToUZ04cUJz587V3Llzy93mlxOPfzkBe+/evZLcgaoi2dnZKiws1KlTp3TJJZeUWd++ffty/5IqT3BwsN58802NHDlSgwYN0qpVqyqc8Fyq9C/sJ598Ulu3blWzZs3OeZ4HH3xQL774oqZNm2Z5btCpU6eUnZ1tadtfCg4OltPprHSbESNGaMSIEcrJydGmTZs0f/58LV68WIMHD9aOHTs8E3iXLl2qoqIidenSRfv27fPsn5ycrEWLFpUbbHr27FlpEO3Ro0e5jxMoz2uvvaYbbrhBt99+e5l15+pr6fvphhtuKPfYERERlmoozy/fu+np6ZLc779f6tixo1auXFlmQv3ZoV+S5710/Pjxc9Z20UUXlXlemdPpVHx8fJllpccsderUKc2aNUvz5s3TwYMHvW65lfee++XvWVhYmFq2bFnm9mPbtm3L1NSuXTtJ7rlhsbGxVRpPs2bNtH37ds/r9PR0tWnTpsx2bdu2rfC4qL8ITmjySuej/PrXv64w+FxxxRVer8/+l+/Zx3juuefUuXPnco8RFhamwsLC86z2jOHDh+v48eP63e9+p1tvvVUff/zxOT8dVfoX9vTp08udu/VLpWFr2rRplq86LV26VGPHjrW07S+NHj260gn+Z4uIiFDfvn3Vt29fBQQEaMGCBdq0aZN69eolSZ65TBVNJv/+++/LnUxfmWnTpp1zXt3+/fs1Y8YMtW/fXtddd12525yrr6Xvp7fffrvcv7T9/c/80W2z2crM2ZHcVyXL88v3bnX4+fmVu7y8Oqzua+WYv//97zVv3jw99NBD6t69u5xOp2w2m4YPH17hvLLadj69QMNEcEKTUt6TuaOjoxUeHq6SkhL16dOnWsctnZgdERFR6TGio6MVHBzsuaJwtt27d1f5vL/97W+VlZWlxx57TL/+9a+1ZMmSSh+LcPZf2JVdHTvbQw89pJdeeknTp0+39Dyjfv36VfsWalxcXLX269q1qxYsWKDDhw9LcoeXDRs26IEHHvAEqVIul0t33323Fi9erMcee6xK57npppsqXf/jjz+qZ8+eatu2rdauXasWLVpUuG1lfS19P8XExJzzPdmsWTOv20KlSq8knUtCQoKk8t9/3377rS644IJzXs2sK++++65Gjx6t559/3rOsoKCgwgn8e/fu1fXXX+95nZeXp8OHD2vgwIFe2+3bt0/GGK8/H/bs2SNJZZ6JVh0JCQnatWtXmXOcfSUUDQfBCU1KaGhomT9k/fz8dNttt2nx4sXasWOHLrvsMq/1R48e9ZoTUp6kpCS1adNGs2fP1l133VXmIYqlx/Dz81O/fv30wQcf6IcffvDc8khLS9PKlSurNaZHH31UP//8s1588UU5nU698cYblW5f+hf2U089Zen4Z4etK6+88pzbt2zZUi1btrR07KrIz8/X119/re7du5dZV/oJptLbTaVXmx555JEyt4Ak6c0339SiRYuqHJzO5c0335TNZtPatWvP2YPK+tqvXz9FRETo6aef1vXXX1/mwa5nvyfbtGmj5cuXey37+uuvtX79+nLH/kstW7ZU586dtWDBAk2dOtUT4nbs2KH//Oc/XvN3fM3Pz6/MlZw5c+ZUeHVt7ty5Gjt2rKd/r732moqLi8s8BPXQoUN6//33deutt0qScnJytHDhQnXu3LnS23RWlf5j4qOPPtLNN98syR34zvW7ivqJ4IQmJSkpSatXr9YLL7yguLg4tWrVSsnJyfrzn/+sdevWKTk5WePHj9ell16qrKwsffXVV1q9evU5P/5ut9v15ptvasCAAerUqZPGjh2rCy+8UAcPHtS6desUERHheVr09OnTtWLFCl133XX63e9+p+LiYs2ZM0edOnXymhdRFc8//7yOHz+uN998U1FRUXrmmWcq3NbpdOrBBx+0PElcOnOL7+uvv/bZ1Yf8/Hz16NFDV199tfr376/4+HidOHFCH3zwgb744gsNHTpUXbp0keQOTp07d64wOAwZMkS///3vyzyI8t133y33yeF9+/at9OpRqWnTpun+++8v95lS5amorxEREXrttdd0991366qrrtLw4cMVHR2tH374QZ988omuueYavfrqq5Kke+65Ry+88IL69euncePGKTMzU6+//ro6depkabK25L7FPGDAAHXv3l3jxo3TqVOnNGfOHDmdznr1LK+bbrpJb7/9tpxOpy699FJt3LhRq1evVvPmzcvd/vTp00pJSdGwYcO0e/du/fWvf9W1117rNUlbcs9nGjdunLZs2aIWLVrorbfe0pEjR7w+GHA+7r33Xr366qsaMWKEHnzwQbVs2VKLFi3yzMfjOyobGF99nA+oitKPF2/ZsqXc9VYfR/Dtt9+anj17muDgYCPJ69EER44cMffff7+Jj483AQEBJjY21qSkpJi5c+d6tjn7Y+Xl2bp1q7n11ltN8+bNjcPhMAkJCWbYsGFmzZo1Xtt99tlnJikpyQQGBprWrVub119/vdx6y1PRIwGKi4vN0KFDjSQza9YsY4z34wjOdvz4ceN0Oit9HMEvldbnq8cRFBUVmTfeeMMMHTrUJCQkGIfDYUJCQkyXLl3Mc889ZwoLC40xxqSmphpJ5vHHH6/wWAcOHDCSzMMPP2yMqfxxBKqBj4tXt6/r1q0z/fr1M06n0wQFBZk2bdqYMWPGmC+//NJru3feece0bt3aBAYGms6dO5uVK1dW+DiCs/97n2316tXmmmuuMcHBwSYiIsIMHjzY7Nq1q9xajx496rW89Pdz//79lfahovdjQkKCGTRoUJnlksz999/veX38+HEzduxYc8EFF5iwsDDTr18/8+2335qEhASv3+XSej777DMzYcIE06xZMxMWFmZGjhzp9ciFs8+9cuVKc8UVVxiHw2E6dOhQ5r9VRY8jKG88v+y9McZ8//33ZtCgQSY4ONhER0ebyZMnm/fee89I8noMA+o/mzHMYAMANB7z58/X2LFjtWXLlnN+CjcxMVGXXXaZli1bVkfVnfHSSy/p4Ycf1k8//aQLL7ywzs+P6uE5TgAA1LJTp055vS4oKNDf/vY3XXLJJYSmBoY5TgAA1LJbb71VF198sTp37qzs7Gy98847+vbbbyv9CiDUTwQnAABqWb9+/Tyf5iwpKdGll16qJUuW6M477/R1aagi5jgBAABYxBwnAAAAiwhOAAAAFjHH6RxcLpcOHTqk8PBwHlIGAEAjZIxRbm6u4uLiKv3aKongdE6HDh2y9LUFAACgYfvxxx910UUXVboNwekcwsPDJbmbGRER4eNqaobL5fJ8r9W5knVjRh/c6IMbfTiDXrjRB7em0IecnBzFx8d7/s6vDMHpHEpvz0VERDSq4FRQUKCIiIhG+0tgBX1wow9u9OEMeuFGH9yaUh+sTMlp3B0AAACoQQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABbxXXVoslwuo5+O5+vAySxFhgaqXUy47PZzf08RAKDpIjihSUpNz9LCDQd0Ove49uXaFeDvr7YxYRrdI0FJCVG+Lg8AUE9xqw5NTmp6lmZ+kqYdB7MVHOini5qFKCLIXzsPZWvmJ2lKTc/ydYkAgHqK4IQmxeUyWrAhXSfyi5TYPERBAX7ys9sU6vBXQlSIsk8VaeGGdLlcxtelAgDqIYITmpQ9mbnal5mnmHCHbDbv+Uw2m03RYQ7tzczTnsxcH1UIAKjPCE5oUrLzi3S6uERBAX7lrg8K8NPp4hJl5xfVcWUAgIaA4IQmxRkSoEB/PxUUlZS7vqCoRIH+fnKGBNRxZQCAhoDghCalXUy42saE6WheoYzxnsdkjNHRvEJdEhOmdjHhPqoQAFCfEZzQpNjtNo3ukSBncIDSs/JVUFSiEpfRycJipWflyxkcoFE9EnieEwCgXAQnNDlJCVF6dFBHdYpz6tTpEv10PF85BcW6LM6pRwd15DlOAIAK8QBMNElJCVG68kKntu1NV3FgBE8OBwBYQnBCk2W323RRsxDFxETJbufiKwDg3PjbAgAAwCKCEwAAgEUEJwAAAIsITgAAABY1uOD0l7/8RYmJiQoKClJycrI2b95c6fb//Oc/1aFDBwUFBenyyy/X8uXL66hSAADQ2DSo4LR06VJNmjRJTz75pL766itdeeWV6tevnzIzM8vdfsOGDRoxYoTGjRunrVu3aujQoRo6dKh27NhRx5UDAIDGoEEFpxdeeEHjx4/X2LFjdemll+r1119XSEiI3nrrrXK3f/nll9W/f3/97//+rzp27KgZM2boqquu0quvvlrHlQMAgMagwQSn06dPKzU1VX369PEss9vt6tOnjzZu3FjuPhs3bvTaXpL69etX4fYAAACVaTAPwDx27JhKSkrUokULr+UtWrTQt99+W+4+GRkZ5W6fkZFR4XkKCwtVWFjoeZ2TkyNJcrlccrlc1S2/XnG5XDLGNJrxVBd9cKMPbvThDHrhRh/cmkIfqjK2BhOc6sqsWbM0ffr0MsuPHj2qgoICH1RU81wul7Kzs2WMadJPzKYPbvTBjT6cQS/c6INbU+hDbm6u5W0bTHC64IIL5OfnpyNHjngtP3LkiGJjY8vdJzY2tkrbS9LUqVM1adIkz+ucnBzFx8crOjpaERER5zGC+sPlcslmsyk6OrrR/hJYQR/c6IMbfTiDXrjRB7em0IegoCDL2zaY4BQYGKikpCStWbNGQ4cOleT+j7lmzRo98MAD5e7TvXt3rVmzRg899JBn2apVq9S9e/cKz+NwOORwOMost9vtjeoNY7PZGt2YqoM+uNEHN/pwBr1wow9ujb0PVRlXgwlOkjRp0iSNHj1aXbt2Vbdu3fTSSy/p5MmTGjt2rCRp1KhRuvDCCzVr1ixJ0oMPPqhevXrp+eef16BBg7RkyRJ9+eWXmjt3ri+HAQAAGqgGFZzuvPNOHT16VE888YQyMjLUuXNnrVixwjMB/IcffvBKjT169NDixYv12GOP6Y9//KMuueQSffDBB7rssst8NQQAANCANajgJEkPPPBAhbfmPv300zLL7rjjDt1xxx21XBUAAGgKGufNSgAAgFpAcAIAALCI4AQAAGARwQkAAMAighMAAIBFBCcAAACLCE4AAAAWEZwAAAAsIjgBAABYRHACAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABYRnAAAACwiOAEAAFhEcAIAALCI4AQAAGARwQkAAMAighMAAIBFBCcAAACLCE4AAAAW+fu6AAAA0DC4XEZ7MnOVnV8kZ0iA2sWEy263+bqsOkVwAgAA5/TVD8e1cOMP2peZp9PFJQr091PbmDCN7pGgpIQoX5dXZ7hVBwAAKrUvM1ezlqdpx8FsRQT566JmIYoI8tfOQ9ma+UmaUtOzfF1inSE4AQCACrlcRmu/zdSJ/CIlNg9RqMNffnabQh3+SogKUfapIi3ckC6Xy/i61DpBcAIAABXaezRXh08UKCbcIZvNez6TzWZTdJhDezPztCcz10cV1i2CEwAAqFB2frGKSlwKCvArd31QgJ9OF5coO7+ojivzDYITAACokDPEXwF+dhUUlZS7vqDIPVHcGRJQx5X5BsEJAABU6JLocLWMDNLRvEIZ4z2PyRijo3mFuiQmTO1iwn1UYd0iOAEAgArZ7Tbd0CFGzuAApWfl62RhsUpcRicLi5WelS9ncIBG9UhoMs9zIjgBAIBKtY0J19SBHdUpzqmcgmL9dDxfOQXFuizOqUcHdWxSz3HiAZgAAOCcrrq4ma66OIonh/u6AAAA0DDY7TZ1iI3wdRk+xa06AAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABYRnAAAACwiOAEAAFhEcAIAALCowQSnrKwsjRw5UhEREYqMjNS4ceOUl5dX6T5z585V7969FRERIZvNphMnTtRNsQAAoFFqMMFp5MiR2rlzp1atWqVly5bp888/14QJEyrdJz8/X/3799cf//jHOqoSAAA0Zv6+LsCKtLQ0rVixQlu2bFHXrl0lSXPmzNHAgQM1e/ZsxcXFlbvfQw89JEn69NNP66hSAADQmDWIK04bN25UZGSkJzRJUp8+fWS327Vp0yYfVgYAAJqSBnHFKSMjQzExMV7L/P39FRUVpYyMjBo9V2FhoQoLCz2vc3JyJEkul0sul6tGz+UrLpdLxphGM57qog9u9MGNPpxBL9zog1tT6ENVxubT4DRlyhQ988wzlW6TlpZWR9W4zZo1S9OnTy+z/OjRoyooKKjTWmqLy+VSdna2jDGy2xvERcdaQR/c6IMbfTiDXrjRB7em0Ifc3FzL2/o0OE2ePFljxoypdJvWrVsrNjZWmZmZXsuLi4uVlZWl2NjYGq1p6tSpmjRpkud1Tk6O4uPjFR0drYiIiBo9l6+4XC7ZbDZFR0c32l8CK+iDG31wow9n0As3+uDWFPoQFBRkeVufBqfo6GhFR0efc7vu3bvrxIkTSk1NVVJSkiRp7dq1crlcSk5OrtGaHA6HHA5HmeV2u71RvWFsNlujG1N10Ac3+uBGH86gF270wa2x96Eq42oQHejYsaP69++v8ePHa/PmzVq/fr0eeOABDR8+3POJuoMHD6pDhw7avHmzZ7+MjAxt27ZN+/btkyR988032rZtm7KysnwyDgAA0LA1iOAkSYsWLVKHDh2UkpKigQMH6tprr9XcuXM964uKirR7927l5+d7lr3++uvq0qWLxo8fL0nq2bOnunTpoo8++qjO6wcAAA1fg/hUnSRFRUVp8eLFFa5PTEyUMcZr2bRp0zRt2rRargwAADQVDeaKEwAAgK8RnAAAACwiOAEAAFhEcAIAALCI4AQAAGARwQkAAMAighMAAIBFBCcAAACLCE4AAAAWEZwAAAAsIjgBAABYRHACAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEX+vi4AdcPlMtqTmavs/CJFBPvJaYxPzu0MCVC7mHDZ7bY6Oz8AADWF4NQEpKZnacGGdO3LzNPp4hI5/O1Kbumngd0C1TWxeZ2eO9DfT21jwjS6R4KSEqJq9dwAANQ0btU1cqnpWZr5SZp2HMxWRJC/LmoWooggf/3wc75mLU9TanpWnZ9756Fszfykds8NAEBtIDg1Ui6X0a7D2XrhP3uUmVuohOYhCnX4y89uU6jDX9HhDmWfKtLCDelyuWr+tp3LZbRgQ7pO5Bcp8RfnTogKqdVzAwBQW7hV1wiV3h7bcTBbPx3Pl7/drl0lObqoWYgigwMkSTabTdFhDu3NzNOezFx1iI2o0Rr2ZOZqX2aeYsIdstm85zPV9rkBAKgtXHFqZM6+PRbkb5e/3a4AP5tyC4q190iuTpwq8mwbFOCn08Ulys4vquSI1ZOdX6TTxSUKCvArd31tnhsAgNpCcGpEfnl7LDwoQHa7+wpPcICfil1GPx3PV+nNsYIi92RtZ0hAjdfiDAlQoL+fCopKyl1fm+cGAKC2EJwakV/eHgt1+Ck00F+nS1ySMQr0s+tkYYlOFhbLGKOjeYW6JCZM7WLCa7yWdjHhahsTpqN5hTK/ePRBbZ8bAIDaQnBqRH55e8xms+miZiHyt9t0qtglSSpxuZRbUKSjuYVyBgdoVI+EWnmmkt1u0+geCXIGByg9K18nC4tV4jI6WVis9Kz8Wj03AAC1heDUiJR3eywyJECXxIQr3OGv08UulbikgmKXLm4eoqkDO9bqs5SSEqL06KCO6hTnVE5BsX46nq+cgmJdFufUo4Nq99wAANQGPlXXiJTeHtt5KFshgX6eT7NFhgQoIihce4+eVEJUiP44sL2a2QoUG9us1mtKSohSl/hmPDkcANAocMWpEans9tgPx08pJtyhSTe2U8eWzjoNLna7TR1iI5Tcurk6xEYQmgAADRbBqZHh9hgAALWHW3WNELfHAACoHQSnRqr09hgAAKg53KoDAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWMQDMH3E5TI82RsAgAaG4OQDqelZWrAhXfsy83S6uESB/n5qGxOm0T0S+C45AADqMW7V1bHU9CzN/CRNOw5mKyLIXxc1C1FEkL92HsrWzE/SlJqe5esSAQBABQhOdcjlMlqwIV0n8ouU2DxEoQ5/+dltCnX4KyEqRNmnirRwQ7pcLuPrUgEAQDkITnVoT2au9mXmKSbcIZvNez6TzWZTdJhDezPztCcz10cVAgCAyhCc6lB2fpFOF5coKMCv3PVBAX46XVyi7PyiOq4MAABYQXCqQ86QAAX6+6mgqKTc9QVF7onizpCAOq4MAABYQXCqQ+1iwtU2JkxH8wpljPc8JmOMjuYV6pKYMLWLCfdRhQAAoDIEpzpkt9s0ukeCnMEBSs/K18nCYpW4jE4WFis9K1/O4ACN6pHA85wAAKinCE51LCkhSo8O6qhOcU7lFBTrp+P5yiko1mVxTj06qCPPcQIAoB7jAZg+kJQQpS7xzXhyOAAADUyDueKUlZWlkSNHKiIiQpGRkRo3bpzy8vIq3f73v/+92rdvr+DgYF188cWaOHGisrOz67DqitntNnWIjVBy6+bqEBtBaAIAoAFoMMFp5MiR2rlzp1atWqVly5bp888/14QJEyrc/tChQzp06JBmz56tHTt2aP78+VqxYoXGjRtXh1UDAIDG5Lxu1RUUFCgoKKimaqlQWlqaVqxYoS1btqhr166SpDlz5mjgwIGaPXu24uLiyuxz2WWX6b333vO8btOmjWbOnKlf//rXKi4ulr8/dykBAEDVVDk9uFwuzZw5U6+//rqOHDmiPXv2qHXr1nr88ceVmJhYK1d0Nm7cqMjISE9okqQ+ffrIbrdr06ZNuuWWWywdJzs7WxEREZWGpsLCQhUWFnpe5+TkSHKP2+VyVXME9YvL5ZIxptGMp7rogxt9cKMPZ9ALN/rg1hT6UJWxVTk4/elPf9KCBQv07LPPavz48Z7ll112mV566aVaCU4ZGRmKiYnxWubv76+oqChlZGRYOsaxY8c0Y8aMSm/vSdKsWbM0ffr0MsuPHj2qgoIC60XXYy6XS9nZ2TLGyG5vMHdraxx9cKMPbvThDHrhRh/cmkIfcnOtf9VZlYPTwoULNXfuXKWkpOi+++7zLL/yyiv17bffVulYU6ZM0TPPPFPpNmlpaVUtsYycnBwNGjRIl156qaZNm1bptlOnTtWkSZO89o2Pj1d0dLQiIiLOu5b6wOVyub8bLzq60f4SWEEf3OiDG304g1640Qe3ptCHqkw7qnJwOnjwoNq2bVtmucvlUlFR1b5jbfLkyRozZkyl27Ru3VqxsbHKzMz0Wl5cXKysrCzFxsZWun9ubq769++v8PBwvf/++woIqPzrTBwOhxwOR5nldru9Ub1hbDZboxtTddAHN/rgRh/OoBdu9MGtsfehKuOqcnC69NJL9cUXXyghIcFr+bvvvqsuXbpU6VjR0dGKjo4+53bdu3fXiRMnlJqaqqSkJEnS2rVr5XK5lJycXOF+OTk56tevnxwOhz766KM6mcgOAAAaryoHpyeeeEKjR4/WwYMH5XK59K9//Uu7d+/WwoULtWzZstqoUR07dlT//v01fvx4vf766yoqKtIDDzyg4cOHez5Rd/DgQaWkpGjhwoXq1q2bcnJydOONNyo/P1/vvPOOcnJyPBO9o6Oj5efnVyu1AgCAxqvK19xuvvlmffzxx1q9erVCQ0P1xBNPKC0tTR9//LH69u1bGzVKkhYtWqQOHTooJSVFAwcO1LXXXqu5c+d61hcVFWn37t3Kz8+XJH311VfatGmTvvnmG7Vt21YtW7b0/Pz444+1VicAAGi8qnTFqbi4WE8//bTuuecerVq1qrZqKldUVJQWL15c4frExEQZYzyve/fu7fUaAADgfFXpipO/v7+effZZFRcX11Y9AAAA9VaVb9WlpKTos88+q41aUE+5XEbfZuRo0/c/69uMHLlcXMkDADRNVZ4cPmDAAE2ZMkXffPONkpKSFBoa6rV+yJAhNVYcfC81PUsLNqRrX2aeTheXKNDfT21jwjS6R4KSEqJ8XR4AAHWqysHpd7/7nSTphRdeKLPOZrOppKTk/KtCvZCanqWZn6TpRH6RYsIdCgpwqKCoRDsPZWvmJ2l6dFBHwhMAoEmp1nfVofFzuYwWbEjXifwiJTYPkc1mkySFOvwVEuin9Kx8LdyQri7xzWS323xcLQCgMXO5jPZk5io7v0jOkAC1iwn32d89VQ5OaBr2ZOZqX2aeYsIdntBUymazKTrMob2ZedqTmasOsY3jq2gAAPVPfZsyUq1np3/22WcaPHiw2rZtq7Zt22rIkCH64osvaro2+FB2fpFOF5coKKD8B4UGBfjpdHGJsvOr9jU7AABYVTplZMfBbEUE+euiZiGKCPL3TBlJTc+q85qqHJzeeecd9enTRyEhIZo4caImTpyo4OBgpaSkVPqcJTQszpAABfr7qaCo/DlrBUXu1O8Mqfy7/wAAqI5fThkJdfjLz25TqMNfCVEhyj5VpIUb0uv8k95VvlU3c+ZMPfvss3r44Yc9yyZOnKgXXnhBM2bM0F133VWjBcI32sWEq21MmHYeylZIoJ/X7TpjjI7mFeqyOKfaxYT7sEoAQGNVX6eMVPmK0/fff6/BgweXWT5kyBDt37+/RoqC79ntNo3ukSBncIDSs/J1srBYJS6jk4XFSs/KlzM4QKN6JDAxHABQK+rrlJEqB6f4+HitWbOmzPLVq1crPj6+RopC/ZCUEKVHB3VUpzincgqK9dPxfOUUFOuyOCePIgAA1Kr6OmWkyrfqJk+erIkTJ2rbtm3q0aOHJGn9+vWaP3++Xn755RovEL6VlBClLvHN6s3HQAEATUN9nTJS5eD029/+VrGxsXr++ef1j3/8Q5LUsWNHLV26VDfffHONFwjfs9ttPHIAAFCnSqeMzPwkTelZ+YoOcygowH0F6mheoc+mjFTrOU633HKLbrnllpquBQAAwKN0ykjpc5yO5RUq0N9Pl8U5NcpHz3GqcnDasmWLXC6XkpOTvZZv2rRJfn5+6tq1a40VBwAAmrb6NmWkypPD77//fv34449llh88eFD3339/jRQFAABQqnTKSHLr5uoQG+HTebZVDk67du3SVVddVWZ5ly5dtGvXrhopCgAAoD6qcnByOBw6cuRImeWHDx+Wvz9ffQcAABqvKgenG2+8UVOnTlV2drZn2YkTJ/THP/5Rffv2rdHiAAAA6pMqXyKaPXu2evbsqYSEBHXp0kWStG3bNrVo0UJvv/12jRcIAABQX1Q5OF144YXavn27Fi1apK+//lrBwcEaO3asRowYoYAAvvAVAAA0XtWalBQaGqoJEybUdC0AAAD1muU5Tnv27NHmzZu9lq1Zs0bXX3+9unXrpqeffrrGiwMAAKhPLAenP/zhD1q2bJnn9f79+zV48GAFBgaqe/fumjVrll566aXaqBEAAKBesHyr7ssvv9Qjjzzieb1o0SK1a9dOK1eulCRdccUVmjNnjh566KEaLxIAAKA+sHzF6dixY7rooos8r9etW6fBgwd7Xvfu3VsHDhyo0eIAAADqE8vBKSoqSocPH5YkuVwuffnll7r66qs960+fPi1jTM1XCAAAUE9YDk69e/fWjBkz9OOPP+qll16Sy+VS7969Pet37dqlxMTEWigRAACgfrA8x2nmzJnq27evEhIS5Ofnp1deeUWhoaGe9W+//bZuuOGGWikSAACgPrAcnBITE5WWlqadO3cqOjpacXFxXuunT5/uNQcKAACgsanSAzD9/f115ZVXlruuouUAAACNRZW/5BcAAKCpIjgBAABYRHACAACwiOAEAABgkaXJ4du3b7d8wCuuuKLaxQAAANRnloJT586dZbPZKnwyeOk6m82mkpKSGi0QAACgvrAUnPbv31/bdQAAANR7loJTQkJCbdcBAABQ71kKTh999JHlAw4ZMqTaxQAAANRnloLT0KFDLR2MOU4AAKAxsxScXC5XbdcBAABQ7/EcJwAAAIssXXF65ZVXNGHCBAUFBemVV16pdNuJEyfWSGEAAAD1jaXg9OKLL2rkyJEKCgrSiy++WOF2NpuN4AQAABqtKj/HiWc6AQCApqrKc5x27NhR4boPPvjgfGoBAACo16ocnPr161fuVaf33ntPI0eOrJGiAAAA6qMqB6ff/OY36tOnjzIyMjzLli5dqlGjRmn+/Pk1WRsAAEC9YmmO09mmT5+urKws9enTR59//rlWrFih3/zmN3r77bd122231UaNAAAA9UKVg5MkzZkzRyNHjtTVV1+tgwcP6u9//7tuvvnmmq4NAACgXqn2d9Xdeuut+uKLLzRixAjZbDbPNnxXHQAAaKzO+7vq3nrrLb311luS+K46AADQuPFddQAAABbxXXUAAAAWWQ5OGzdu1LJly7yWLVy4UK1atVJMTIwmTJigwsLCGi8QAACgvrAcnJ566int3LnT8/qbb77RuHHj1KdPH02ZMkUff/yxZs2aVStFSlJWVpZGjhypiIgIRUZGaty4ccrLy6t0n3vvvVdt2rRRcHCwoqOjdfPNN+vbb7+ttRoBAEDjZjk4bdu2TSkpKZ7XS5YsUXJyst544w1NmjRJr7zyiv7xj3/USpGSNHLkSO3cuVOrVq3SsmXL9Pnnn2vChAmV7pOUlKR58+YpLS1NK1eulDFGN954IxPYAQBAtVh+jtPx48fVokULz+vPPvtMAwYM8Lz+1a9+pR9//LFmq/v/paWlacWKFdqyZYu6du0qyf0sqYEDB2r27NmKi4srd7+zg1ViYqL+9Kc/6corr9SBAwfUpk2bWqkVAAA0XpaDU4sWLbR//37Fx8fr9OnT+uqrrzR9+nTP+tzcXAUEBNRKkRs3blRkZKQnNElSnz59ZLfbtWnTJt1yyy3nPMbJkyc1b948tWrVSvHx8RVuV1hY6DVXKycnR5L7k4WN5dOFLpdLxphGM57qog9u9MGNPpxBL9zog1tT6ENVxmY5OA0cOFBTpkzRM888ow8++EAhISG67rrrPOu3b99ea1dxMjIyFBMT47XM399fUVFRXt+ZV56//vWveuSRR3Ty5Em1b99eq1atUmBgYIXbz5o1yysQljp69KgKCgqqN4B6xuVyKTs7W8YY2e1N94OV9MGNPrjRhzPohRt9cGsKfcjNzbW8reXgNGPGDN16663q1auXwsLCtGDBAq8A8tZbb+nGG2+sUqGlQawyaWlpVTrmL40cOVJ9+/bV4cOHNXv2bA0bNkzr169XUFBQudtPnTpVkyZN8rzOyclRfHy8oqOjFRERcV611Bcul0s2m03R0dGN9pfACvrgRh/c6MMZ9MKNPrg1hT5UlAnKYzk4XXDBBfr888+VnZ2tsLAw+fn5ea3/5z//qbCwMOtVSpo8ebLGjBlT6TatW7dWbGysMjMzvZYXFxcrKytLsbGxle7vdDrldDp1ySWX6Oqrr1azZs30/vvva8SIEeVu73A45HA4yiy32+2N6g1js9ka3Ziqgz640Qc3+nAGvXCjD26NvQ9VGVeVv+TX6XSWuzwqKqqqh1J0dLSio6PPuV337t114sQJpaamKikpSZK0du1auVwuJScnWz6fMUbGGJ43BQAAqqVBRMeOHTuqf//+Gj9+vDZv3qz169frgQce0PDhwz2fqDt48KA6dOigzZs3S5K+//57zZo1S6mpqfrhhx+0YcMG3XHHHQoODtbAgQN9ORwAANBANYjgJEmLFi1Shw4dlJKSooEDB+raa6/V3LlzPeuLioq0e/du5efnS3Lfr/ziiy80cOBAtW3bVnfeeafCw8O1YcOGMhPNAQAArKjyrTpfiYqK0uLFiytcn5iYKGOM53VcXJyWL19eF6UBAIAmosFccQIAAPA1ghMAAIBFBCcAAACLCE4AAAAWEZwAAAAsIjgBAABYRHACAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABYRnAAAACwiOAEAAFhEcAIAALCI4AQAAGARwQkAAMAighMAAIBFBCcAAACLCE4AAAAWEZwAAAAsIjgBAABYRHACAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAi/x9XQCApsvlMtqTmavs/CI5QwLULiZcdrvN12UBQIUITgB8IjU9Sws2pGtfZp5OF5co0N9PbWPCNLpHgpISonxdHgCUi1t1AOpcanqWZn6Sph0HsxUR5K+LmoUoIshfOw9la+YnaUpNz/J1iQBQLoITgDrlchkt2JCuE/lFSmweolCHv/zsNoU6/JUQFaLsU0VauCFdLpfxdakAUAbBCUCd2pOZq32ZeYoJd8hm857PZLPZFB3m0N7MPO3JzPVRhQBQMYITgDqVnV+k08UlCgrwK3d9UICfTheXKDu/qI4rA4BzIzgBqFPOkAAF+vupoKik3PUFRe6J4s6QgDquDADOjeAEoE61iwlX25gwHc0rlDHe85iMMTqaV6hLYsLULibcRxUCQMUITgDqlN1u0+geCXIGByg9K18nC4tV4jI6WVis9Kx8OYMDNKpHAs9zAlAvEZwA1LmkhCg9OqijOsU5lVNQrJ+O5yunoFiXxTn16KCOPMcJQL3FAzAB+ERSQpS6xDfjyeEAGhSCEwCfsdtt6hAb4esyAMAybtUBAABYRHACAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWNRgglNWVpZGjhypiIgIRUZGaty4ccrLy7O0rzFGAwYMkM1m0wcffFC7hQIAgEarwQSnkSNHaufOnVq1apWWLVumzz//XBMmTLC070svvSSbje+/AgAA56dBfFddWlqaVqxYoS1btqhr166SpDlz5mjgwIGaPXu24uLiKtx327Ztev755/Xll1+qZcuWdVUyAABohBpEcNq4caMiIyM9oUmS+vTpI7vdrk2bNumWW24pd7/8/Hzddddd+stf/qLY2FhL5yosLFRhYaHndU5OjiTJ5XLJ5XKdxyjqD5fLJWNMoxlPddEHN/rgRh/OoBdu9MGtKfShKmNrEMEpIyNDMTExXsv8/f0VFRWljIyMCvd7+OGH1aNHD918882WzzVr1ixNnz69zPKjR4+qoKDAetH1mMvlUnZ2towxstsbzN3aGkcf3OiDG304g1640Qe3ptCH3Nxcy9v6NDhNmTJFzzzzTKXbpKWlVevYH330kdauXautW7dWab+pU6dq0qRJntc5OTmKj49XdHS0IiIiqlVLfeNyuWSz2RQdHd1ofwmsoA9u9MGNPpxBL9zog1tT6ENQUJDlbX0anCZPnqwxY8ZUuk3r1q0VGxurzMxMr+XFxcXKysqq8Bbc2rVr9d133ykyMtJr+W233abrrrtOn376abn7ORwOORyOMsvtdnujesPYbLZGN6bqoA9u9MGNPpxBL9zog1tj70NVxuXT4BQdHa3o6Ohzbte9e3edOHFCqampSkpKkuQORi6XS8nJyeXuM2XKFP3mN7/xWnb55ZfrxRdf1ODBg8+/eAAA0OQ0iDlOHTt2VP/+/TV+/Hi9/vrrKioq0gMPPKDhw4d7PlF38OBBpaSkaOHCherWrZtiY2PLvRp18cUXq1WrVnU9BAAA0Ag0mGtuixYtUocOHZSSkqKBAwfq2muv1dy5cz3ri4qKtHv3buXn5/uwSgAA0Jg1iCtOkhQVFaXFixdXuD4xMVHGmEqPca71AAAAlWkwV5wAAAB8jeAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABYRnAAAACwiOAEAAFhEcAIAALCI4AQAAGARwQkAAMAighMAAIBFBCcAAACLCE4AAAAWEZwAAAAsIjgBAABYRHACAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABYRnAAAACwiOAEAAFhEcAIAALCI4AQAAGARwQkAAMAighMAAIBFBCcAAACLCE4AAAAWEZwAAAAsIjgBAABYRHACAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWNRgglNWVpZGjhypiIgIRUZGaty4ccrLy6t0n969e8tms3n93HfffXVUMQAAaGz8fV2AVSNHjtThw4e1atUqFRUVaezYsZowYYIWL15c6X7jx4/XU0895XkdEhJS26UCAIBGqkEEp7S0NK1YsUJbtmxR165dJUlz5szRwIEDNXv2bMXFxVW4b0hIiGJjY+uqVAAA0Ig1iOC0ceNGRUZGekKTJPXp00d2u12bNm3SLbfcUuG+ixYt0jvvvKPY2FgNHjxYjz/+eKVXnQoLC1VYWOh5nZOTI0lyuVxyuVw1MBrfc7lcMsY0mvFUF31wow9u9OEMeuFGH9yaQh+qMrYGEZwyMjIUExPjtczf319RUVHKyMiocL+77rpLCQkJiouL0/bt2/WHP/xBu3fv1r/+9a8K95k1a5amT59eZvnRo0dVUFBQ/UHUIy6XS9nZ2TLGyG5vMNPcahx9cKMPbvThDHrhRh/cmkIfcnNzLW/r0+A0ZcoUPfPMM5Vuk5aWVu3jT5gwwfP/L7/8crVs2VIpKSn67rvv1KZNm3L3mTp1qiZNmuR5nZOTo/j4eEVHRysiIqLatdQnLpdLNptN0dHRjfaXwAr64EYf3OjDGfTCjT64NYU+BAUFWd7Wp8Fp8uTJGjNmTKXbtG7dWrGxscrMzPRaXlxcrKysrCrNX0pOTpYk7du3r8Lg5HA45HA4yiy32+2N6g1js9ka3Ziqgz640Qc3+nAGvXCjD26NvQ9VGZdPg1N0dLSio6PPuV337t114sQJpaamKikpSZK0du1auVwuTxiyYtu2bZKkli1bVqteAADQtDWI6NixY0f1799f48eP1+bNm7V+/Xo98MADGj58uOcTdQcPHlSHDh20efNmSdJ3332nGTNmKDU1VQcOHNBHH32kUaNGqWfPnrriiit8ORwAANBANYjgJLk/HdehQwelpKRo4MCBuvbaazV37lzP+qKiIu3evVv5+fmSpMDAQK1evVo33nijOnTooMmTJ+u2227Txx9/7KshAACABq5BfKpOkqKioip92GViYqKMMZ7X8fHx+uyzz+qiNAAA0EQ0mCtOAAAAvkZwAgAAsIjgBAAAYBHBCQAAwKIGMzkcjZ/LZbQnM1fZ+UVyhgSoXUy47Habr8sCAMCD4IR6ITU9Sws2pGtfZp5OF5co0N9PbWPCNLpHgpISonxdHgAAkrhVh3ogNT1LMz9J046D2YoI8tdFzUIUEeSvnYeyNfOTNKWmZ/m6RAAAJBGc4GMul9GCDek6kV+kxOYhCnX4y89uU6jDXwlRIco+VaSFG9LlcplzHwwAgFpGcIJP7cnM1b7MPMWEO2Szec9nstlsig5zaG9mnvZk5vqoQgAAziA4waey84t0urhEQQF+5a4PCvDT6eISZecX1XFlAACURXCCTzlDAhTo76eCopJy1xcUuSeKO0MC6rgyAADKIjjBp9rFhKttTJiO5hV6fdegJBljdDSvUJfEhKldTLiPKgQA4AyCE3zKbrdpdI8EOYMDlJ6Vr5OFxSpxGZ0sLFZ6Vr6cwQEa1SOB5zkBAOoFghN8LikhSo8O6qhOcU7lFBTrp+P5yiko1mVxTj06qCPPcQIA1Bs8ABP1QlJClLrEN+PJ4QCAeo3ghHrDbrepQ2yEr8sAAKBC3KoDAACwiOAEAABgEcEJAADAIoITAACARQQnAAAAiwhOAAAAFhGcAAAALCI4AQAAWERwAgAAsIjgBAAAYBHBCQAAwCK+q+4cjDGSpJycHB9XUnNcLpdyc3MVFBQku73pZmf64EYf3OjDGfTCjT64NYU+lP4dX/p3fmUITueQm5srSYqPj/dxJQAAoDbl5ubK6XRWuo3NWIlXTZjL5dKhQ4cUHh4um83m63JqRE5OjuLj4/Xjjz8qIiLC1+X4DH1wow9u9OEMeuFGH9yaQh+MMcrNzVVcXNw5r6pxxekc7Ha7LrroIl+XUSsiIiIa7S9BVdAHN/rgRh/OoBdu9MGtsffhXFeaSjXOm5UAAAC1gOAEAABgEcGpCXI4HHryySflcDh8XYpP0Qc3+uBGH86gF270wY0+eGNyOAAAgEVccQIAALCI4AQAAGARwQkAAMAiglMD9Je//EWJiYkKCgpScnKyNm/eXOn2//znP9WhQwcFBQXp8ssv1/Lly73WG2P0xBNPqGXLlgoODlafPn20d+9er21mzpypHj16KCQkRJGRkTU9pGqp6z4cOHBA48aNU6tWrRQcHKw2bdroySef1OnTp2tlfFb54v0wZMgQXXzxxQoKClLLli11991369ChQzU+tqryRS9KFRYWqnPnzrLZbNq2bVtNDalafNGHxMRE2Ww2r58///nPNT62qvDV++GTTz5RcnKygoOD1axZMw0dOrQmh1Vldd2HTz/9tMx7ofRny5YttTLGOmXQoCxZssQEBgaat956y+zcudOMHz/eREZGmiNHjpS7/fr1642fn5959tlnza5du8xjjz1mAgICzDfffOPZ5s9//rNxOp3mgw8+MF9//bUZMmSIadWqlTl16pRnmyeeeMK88MILZtKkScbpdNb2MM/JF33497//bcaMGWNWrlxpvvvuO/Phhx+amJgYM3ny5DoZc3l89X544YUXzMaNG82BAwfM+vXrTffu3U337t1rfbyV8VUvSk2cONEMGDDASDJbt26trWGek6/6kJCQYJ566ilz+PBhz09eXl6tj7civurDu+++a5o1a2Zee+01s3v3brNz506zdOnSWh9vRXzRh8LCQq/3weHDh81vfvMb06pVK+Nyuepk3LWJ4NTAdOvWzdx///2e1yUlJSYuLs7MmjWr3O2HDRtmBg0a5LUsOTnZ3HvvvcYYY1wul4mNjTXPPfecZ/2JEyeMw+Ewf//738scb968efUiOPm6D6WeffZZ06pVq/MZynmpL3348MMPjc1mM6dPnz6f4ZwXX/Zi+fLlpkOHDmbnzp0+D06+6kNCQoJ58cUXa3Ak58cXfSgqKjIXXnihefPNN2t6ONVWH/6MOH36tImOjjZPPfXU+Q6nXuBWXQNy+vRppaamqk+fPp5ldrtdffr00caNG8vdZ+PGjV7bS1K/fv082+/fv18ZGRle2zidTiUnJ1d4TF+rT33Izs5WVFTU+Qyn2upLH7KysrRo0SL16NFDAQEB5zusavFlL44cOaLx48fr7bffVkhISE0Oq8p8/Z7485//rObNm6tLly567rnnVFxcXFNDqxJf9eGrr77SwYMHZbfb1aVLF7Vs2VIDBgzQjh07anqIlvj6/VDqo48+0s8//6yxY8ee75DqBYJTA3Ls2DGVlJSoRYsWXstbtGihjIyMcvfJyMiodPvS/63KMX2tvvRh3759mjNnju69995qjeN8+boPf/jDHxQaGqrmzZvrhx9+0Icffnhe4zkfvuqFMUZjxozRfffdp65du9bIWM6HL98TEydO1JIlS7Ru3Trde++9evrpp/XII4+c95iqw1d9+P777yVJ06ZN02OPPaZly5apWbNm6t27t7Kyss5/YFXk6z8jSv2///f/1K9fv0bzva8EJ6AaDh48qP79++uOO+7Q+PHjfV2OT/zv//6vtm7dqv/85z/y8/PTqFGjZJrY83TnzJmj3NxcTZ061del+NykSZPUu3dvXXHFFbrvvvv0/PPPa86cOSosLPR1aXXG5XJJkh599FHddtttSkpK0rx582Sz2fTPf/7Tx9X5xk8//aSVK1dq3Lhxvi6lxhCcGpALLrhAfn5+OnLkiNfyI0eOKDY2ttx9YmNjK92+9H+rckxf83UfDh06pOuvv149evTQ3Llzz2ss58PXfbjgggvUrl079e3bV0uWLNHy5cv13//+97zGVF2+6sXatWu1ceNGORwO+fv7q23btpKkrl27avTo0ec/sCry9XvibMnJySouLtaBAweqOozz5qs+tGzZUpJ06aWXetY7HA61bt1aP/zww3mMqHrqw/th3rx5at68uYYMGVLtcdQ3BKcGJDAwUElJSVqzZo1nmcvl0po1a9S9e/dy9+nevbvX9pK0atUqz/atWrVSbGys1zY5OTnatGlThcf0NV/24eDBg+rdu7fnX5J2u+9+herT+6H0X9q+urrgq1688sor+vrrr7Vt2zZt27bN87HtpUuXaubMmTU6Rivq03ti27ZtstvtiomJOZ8hVYuv+pCUlCSHw6Hdu3d7tikqKtKBAweUkJBQY+OzytfvB2OM5s2bp1GjRvls/mOt8PHkdFTRkiVLjMPhMPPnzze7du0yEyZMMJGRkSYjI8MYY8zdd99tpkyZ4tl+/fr1xt/f38yePdukpaWZJ598styPlkZGRpoPP/zQbN++3dx8881lPmKbnp5utm7daqZPn27CwsLM1q1bzdatW01ubm7dDf4svujDTz/9ZNq2bWtSUlLMTz/95PVRW1/xRR/++9//mjlz5pitW7eaAwcOmDVr1pgePXqYNm3amIKCgrptwFl89btxtv379/v8U3W+6MOGDRvMiy++aLZt22a+++47884775jo6GgzatSouh38WXz1fnjwwQfNhRdeaFauXGm+/fZbM27cOBMTE2OysrLqbvBn8eXvxerVq40kk5aWVjeDrSMEpwZozpw55uKLLzaBgYGmW7du5r///a9nXa9evczo0aO9tv/HP/5h2rVrZwIDA02nTp3MJ5984rXe5XKZxx9/3LRo0cI4HA6TkpJidu/e7bXN6NGjjaQyP+vWrautYZ5TXfdh3rx55fbA1//+qOs+bN++3Vx//fUmKirKOBwOk5iYaO677z7z008/1eo4rfDF78bZ6kNwMqbu+5CammqSk5ON0+k0QUFBpmPHjubpp5/2aZA2xjfvh9OnT5vJkyebmJgYEx4ebvr06WN27NhRa2O0wle/FyNGjDA9evSolTH5ks2YJjabEwAAoJqY4wQAAGARwQkAAMAighMAAIBFBCcAAACLCE4AAAAWEZwAAAAsIjgBAABYRHACAACwiOAEAABgEcEJgE/Mnz9fkZGRvi6jTjz++OOaMGFCnZ/32LFjiomJ0U8//VTn5wYaK4ITgGobM2aMbDab56d58+bq37+/tm/ffs5977zzTu3Zs6dW6urdu7ceeuihWjl2VWVkZOjll1/Wo48+6llWG/WNGTNGQ4cO9Vp2wQUXaNSoUXryySdr9FxAU0ZwAnBe+vfvr8OHD+vw4cNas2aN/P39ddNNN1W6T1FRkYKDgxUTE3Ne5y4qKjqv/evCm2++qR49eighIcEn5x87dqwWLVqkrKwsn5wfaGwITgDOi8PhUGxsrGJjY9W5c2dNmTJFP/74o44ePSpJOnDggGw2m5YuXapevXopKChIixYtKvdW3YcffqirrrpKQUFBat26taZPn67i4mLPepvNptdee01DhgxRaGioZs6cWa2a33vvPXXq1EkOh0OJiYl6/vnnvdb/9a9/1SWXXKKgoCC1aNFCt99+u2fdu+++q8svv1zBwcFq3ry5+vTpo5MnT1Z4riVLlmjw4MGe12PGjNFnn32ml19+2XOl7sCBA5KkHTt2aMCAAQoLC1OLFi10991369ixY+c897Rp07RgwQJ9+OGHnmN++umnkqROnTopLi5O77//frV6BcAbwQlAjcnLy9M777yjtm3bqnnz5l7rpkyZogcffFBpaWnq169fmX2/+OILjRo1Sg8++KB27dqlv/3tb5o/f36ZcDRt2jTdcsst+uabb3TPPfdUucbU1FQNGzZMw4cP1zfffKNp06bp8ccf1/z58yVJX375pSZOnKinnnpKu3fv1ooVK9SzZ09J0uHDhzVixAjdc889SktL06effqpbb71Vxphyz5WVlaVdu3apa9eunmUvv/yyunfvrvHjx3uu1MXHx+vEiRO64YYb1KVLF3355ZdasWKFjhw5omHDhp3z3P/zP/+jYcOGeV3969Gjh+ec3bp10xdffFHlXgEoy9/XBQBo2JYtW6awsDBJ0smTJ9WyZUstW7ZMdrv3v8seeugh3XrrrRUeZ/r06ZoyZYpGjx4tSWrdurVmzJihRx55xGuOzl133aWxY8dWu94XXnhBKSkpevzxxyVJ7dq1065du/Tcc89pzJgx+uGHHxQaGqqbbrpJ4eHhSkhIUJcuXSS5w0txcbFuvfVWz623yy+/vMJz/fDDDzLGKC4uzrPM6XQqMDBQISEhio2N9Sx/9dVX1aVLFz399NOeZW+99Zbi4+O1Z88e5eXlVXru4OBgFRYWeh2zVFxcnLZu3VqddgH4Ba44ATgv119/vbZt26Zt27Zp8+bN6tevnwYMGKD09HSv7c6+6lKer7/+Wk899ZTCwsI8P6VXZfLz8y0f51zS0tJ0zTXXeC275pprtHfvXpWUlKhv375KSEhQ69atdffdd2vRokWe81955ZVKSUnR5ZdfrjvuuENvvPGGjh8/XuG5Tp06JUkKCgo6Z11ff/211q1b5zX+Dh06SJK+++67Kp/7bMHBwV49BFB9BCcA5yU0NFRt27ZV27Zt9atf/UpvvvmmTp48qTfeeKPMdpXJy8vT9OnTPSFs27Zt+uabb7R3716v4HGu45yv8PBwffXVV/r73/+uli1b6oknntCVV16pEydOyM/PT6tWrdK///1vXXrppZozZ47at2+v/fv3l3usCy64QJIsBZy8vDwNHjzYa/zbtm3T3r171bNnzyqf+2xZWVmKjo6uWiMAlIvgBKBG2Ww22e12z9UWq6666irt3r3bE8LO/vnlbb/z0bFjR61fv95r2fr169WuXTv5+flJkvz9/dWnTx89++yz2r59uw4cOKC1a9dKco/vmmuu0fTp07V161YFBgZWOPG6TZs2ioiI0K5du7yWBwYGqqSkxGvZVVddpZ07dyoxMbHM+EvDYmXnLu+YpXbs2OG53Qjg/DDHCcB5KSwsVEZGhiT3lZVXX33Vc/WkKp544gnddNNNuvjii3X77bfLbrfr66+/1o4dO/SnP/2pynUdPXpU27Zt81rWsmVLTZ48Wb/61a80Y8YM3Xnnndq4caNeffVV/fWvf5XknrP1/fffq2fPnmrWrJmWL18ul8ul9u3ba9OmTVqzZo1uvPFGxcTEaNOmTTp69Kg6duxYbg12u119+vTR//3f/3k9YykxMVGbNm3SgQMHFBYWpqioKN1///164403NGLECD3yyCOKiorSvn37tGTJEr355pv68ssvKz13YmKiVq5cqd27d6t58+ZyOp0KCAhQfn6+UlNTveZOATgPBgCqafTo0UaS5yc8PNz86le/Mu+++65nm/379xtJZuvWrV77zps3zzidTq9lK1asMD169DDBwcEmIiLCdOvWzcydO9ezXpJ5//33z1lXr169vOoq/ZkxY4Yxxph3333XXHrppSYgIMBcfPHF5rnnnvPs+8UXX5hevXqZZs2ameDgYHPFFVeYpUuXGmOM2bVrl+nXr5+Jjo42DofDtGvXzsyZM6fSWpYvX24uvPBCU1JS4lm2e/duc/XVV5vg4GAjyezfv98YY8yePXvMLbfcYiIjI01wcLDp0KGDeeihh4zL5TrnuTMzM03fvn1NWFiYkWTWrVtnjDFm8eLFpn379ufsGQBrbMZU8DlaAMB5M8YoOTlZDz/8sEaMGFHn57/66qs1ceJE3XXXXXV+bqAxYo4TANQim82muXPnej3Is64cO3ZMt956q08CG9BYccUJAADAIq44AQAAWERwAgAAsIjgBAAAYBHBCQAAwCKCEwAAgEUEJwAAAIsITgAAABYRnAAAACwiOAEAAFj0/wGP2cC7y2PQBwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# =====================================================================\n",
    "# Allen Mouse Neurons ‚Äî CLIP-SAE-ViT-L-14 ‚Üí Neuron Mapping (Filtered KNN)\n",
    "# =====================================================================\n",
    "\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt, torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers.models.clip.modeling_clip import CLIPVisionModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng(42)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "N_IMAGES = 118\n",
    "N_TRIALS = 50\n",
    "DATA_PATH = \"/home/maria/LuckyMouse/pixel_transformer_neuro/data/processed/hybrid_neural_responses.npy\"\n",
    "IMG_DIR = Path(\"/home/maria/MITNeuralComputation/vit_embeddings/images\")\n",
    "SAE_REPO = \"zer0int/CLIP-SAE-ViT-L-14\"\n",
    "LOCAL_SAE_DIR = \"./clip_sae_vitl14_weights\"\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# HELPERS\n",
    "# ---------------------------------------------------------------\n",
    "def compute_brier(y_true, y_pred, img_ids):\n",
    "    \"\"\"Aggregate trialwise predictions per image and compute Brier loss.\"\"\"\n",
    "    df = pd.DataFrame({\"img\": img_ids, \"y\": y_true, \"p\": y_pred})\n",
    "    agg = df.groupby(\"img\").mean()\n",
    "    return float(np.mean((agg[\"p\"] - agg[\"y\"]) ** 2))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# LOAD NEURAL DATA\n",
    "# ---------------------------------------------------------------\n",
    "dat = np.load(DATA_PATH)\n",
    "Y_binary = (dat > 0).astype(int)\n",
    "n_neurons, n_samples = dat.shape\n",
    "assert n_samples == N_IMAGES * N_TRIALS\n",
    "img_ids_full = np.repeat(np.arange(N_IMAGES), N_TRIALS)\n",
    "print(f\"dat shape (neurons √ó trials): {dat.shape}\")\n",
    "\n",
    "mean_prob = Y_binary.mean()\n",
    "print(f\"Mean prob of spiking: {mean_prob:.4f}\")\n",
    "y_means = Y_binary.mean(axis=1)\n",
    "print(f\"Typical range: {y_means.min():.4f} ‚Äì {y_means.max():.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# DOWNLOAD SAE REPO (if needed)\n",
    "# ---------------------------------------------------------------\n",
    "snapshot_download(repo_id=SAE_REPO, local_dir=LOCAL_SAE_DIR)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# LOAD CLIP MODEL\n",
    "# ---------------------------------------------------------------\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n",
    "clip_model.eval()\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "vision_model: CLIPVisionModel = clip_model.vision_model\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# FIND SAE ENCODER MATRIX\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\nüîç Scanning safetensors for encoder-like matrices...\")\n",
    "candidates = []\n",
    "for f in Path(LOCAL_SAE_DIR).glob(\"*.safetensors\"):\n",
    "    with safe_open(f, framework=\"pt\", device=\"cpu\") as sf:\n",
    "        for k in sf.keys():\n",
    "            t = sf.get_tensor(k)\n",
    "            if t.ndim == 2:\n",
    "                candidates.append((f, k, t.shape))\n",
    "                if t.shape[0]*t.shape[1] > 1_000_000:\n",
    "                    print(f\"  {f.name}: {k} {tuple(t.shape)}\")\n",
    "\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(\"No 2D tensors found.\")\n",
    "sae_path, enc_key, enc_shape = max(candidates, key=lambda x: x[2][0]*x[2][1])\n",
    "print(f\"‚Üí Selected {sae_path.name}, key={enc_key}, shape={enc_shape}\")\n",
    "\n",
    "with safe_open(sae_path, framework=\"pt\", device=DEVICE) as f:\n",
    "    W = f.get_tensor(enc_key)\n",
    "    bias = None\n",
    "    for k in f.keys():\n",
    "        if \"bias\" in k.lower():\n",
    "            b = f.get_tensor(k)\n",
    "            if b.ndim == 1:\n",
    "                bias = b\n",
    "                break\n",
    "\n",
    "# orientation check\n",
    "A, B = W.shape\n",
    "if A > B:\n",
    "    in_dim, out_dim = B, A\n",
    "    W_use = W\n",
    "else:\n",
    "    in_dim, out_dim = A, B\n",
    "    W_use = W.T\n",
    "\n",
    "# bias fix\n",
    "if bias is None or bias.shape[0] != out_dim:\n",
    "    print(f\"‚ö†Ô∏è Bias mismatch ‚Äî creating zero bias of length {out_dim}\")\n",
    "    bias_use = torch.zeros(out_dim, device=DEVICE)\n",
    "else:\n",
    "    bias_use = bias\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# SAE MODULE\n",
    "# ---------------------------------------------------------------\n",
    "class SAEEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, W_init, b_init):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_dim, out_dim)\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.copy_(W_init)\n",
    "            self.linear.bias.copy_(b_init)\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.linear(x))\n",
    "\n",
    "sae = SAEEncoder(in_dim, out_dim, W_use.to(DEVICE), bias_use.to(DEVICE)).to(DEVICE)\n",
    "sae.eval()\n",
    "print(f\"‚úì SAE Linear(in_dim={in_dim}, out_dim={out_dim}) ready\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# EXTRACT SAE FEATURES\n",
    "# ---------------------------------------------------------------\n",
    "img_paths = sorted(IMG_DIR.glob(\"scene_*.png\"))\n",
    "assert len(img_paths) == N_IMAGES\n",
    "print(f\"\\nExtracting SAE features for {N_IMAGES} images...\")\n",
    "\n",
    "feat_fn = lambda inputs: clip_model.get_image_features(**inputs)\n",
    "sae_feats = []\n",
    "for p in tqdm(img_paths, desc=\"Feature extraction\"):\n",
    "    img = Image.open(p).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        x = feat_fn(inputs)\n",
    "        z = sae(x)\n",
    "    sae_feats.append(z.squeeze().cpu().numpy())\n",
    "\n",
    "Z_sae = np.stack(sae_feats)\n",
    "print(\"SAE latent shape:\", Z_sae.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PREPARE DATA\n",
    "# ---------------------------------------------------------------\n",
    "Z_sae_full = np.repeat(Z_sae, N_TRIALS, axis=0)\n",
    "Z_sae_full = StandardScaler().fit_transform(Z_sae_full)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5-FOLD CV ‚Äî FILTERED KNN\n",
    "# ---------------------------------------------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds = list(kf.split(np.arange(N_IMAGES)))\n",
    "\n",
    "num_neurons_to_sample = min(10, n_neurons)\n",
    "sampled_neurons = rng.choice(np.arange(n_neurons), size=num_neurons_to_sample, replace=False)\n",
    "K_NEIGHBORS = 3\n",
    "print(f\"\\nRunning filtered KNN (K={K_NEIGHBORS}) on {num_neurons_to_sample} neurons...\")\n",
    "\n",
    "records = []\n",
    "for nid in tqdm(sampled_neurons, desc=\"Neurons\"):\n",
    "    y_all = Y_binary[nid]\n",
    "    brier_folds, skill_folds = [], []\n",
    "\n",
    "    for train_imgs, test_imgs in folds:\n",
    "        train_mask = np.isin(img_ids_full, train_imgs)\n",
    "        test_mask  = np.isin(img_ids_full, test_imgs)\n",
    "\n",
    "        Z_train, Z_test = Z_sae_full[train_mask], Z_sae_full[test_mask]\n",
    "        y_train, y_test = y_all[train_mask], y_all[test_mask]\n",
    "\n",
    "        # per-image means\n",
    "        y_train_mean = np.array([y_train[img_ids_full[train_mask]==i].mean() for i in train_imgs])\n",
    "        y_test_mean  = np.array([y_test[img_ids_full[test_mask]==i].mean() for i in test_imgs])\n",
    "        Z_train_mean = np.array([Z_train[img_ids_full[train_mask]==i].mean(axis=0) for i in train_imgs])\n",
    "        Z_test_mean  = np.array([Z_test[img_ids_full[test_mask]==i].mean(axis=0) for i in test_imgs])\n",
    "\n",
    "        # ---- FILTER LATENTS ----\n",
    "        support = (Z_train_mean > 0).sum(axis=0)\n",
    "        active1 = support >= 5\n",
    "        stds = Z_train_mean.std(axis=0)\n",
    "        active2 = stds > 1e-6\n",
    "        active_mask = active1 & active2\n",
    "        if active_mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        # IDF weighting\n",
    "        idf = 1.0 / np.log1p(support.clip(min=1))\n",
    "        Z_train_mean *= idf\n",
    "        Z_test_mean  *= idf\n",
    "\n",
    "        # center latents\n",
    "        Z_train_sel = Z_train_mean[:, active_mask]\n",
    "        Z_test_sel  = Z_test_mean[:, active_mask]\n",
    "\n",
    "        # subtract per-latent mean (axis=1 ‚Üí each latent separately)\n",
    "        Z_train_T = Z_train_sel.T - Z_train_sel.T.mean(axis=1, keepdims=True)\n",
    "        Z_test_T  = Z_test_sel.T  - Z_test_sel.T.mean(axis=1, keepdims=True)\n",
    "\n",
    "        # safety check\n",
    "        if np.any(np.isnan(Z_train_T)):\n",
    "            Z_train_T = np.nan_to_num(Z_train_T)\n",
    "        if np.any(np.isnan(Z_test_T)):\n",
    "            Z_test_T = np.nan_to_num(Z_test_T)\n",
    "\n",
    "        # --- KNN search ---\n",
    "        nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric=\"cosine\")\n",
    "        nbrs.fit(Z_train_T)\n",
    "        dist, idx = nbrs.kneighbors(y_train_mean.reshape(1, -1))\n",
    "        best_idxs, weights = idx[0], 1 - dist[0]\n",
    "        weights /= weights.sum()\n",
    "\n",
    "        z_train_pred = np.average(Z_train_T[best_idxs, :], axis=0, weights=weights)\n",
    "        z_test_pred  = np.average(Z_test_T[best_idxs, :], axis=0, weights=weights)\n",
    "\n",
    "        # ---- Linear calibration ----\n",
    "        var_z = np.var(z_train_pred)\n",
    "        if var_z < 1e-8:\n",
    "            a, b = 0.0, y_train_mean.mean()\n",
    "        else:\n",
    "            a = np.cov(y_train_mean, z_train_pred)[0,1] / var_z\n",
    "            b = y_train_mean.mean() - a * z_train_pred.mean()\n",
    "        y_pred_test = np.clip(a * z_test_pred + b, 0, 1)\n",
    "\n",
    "        # ---- Evaluate ----\n",
    "        brier = float(np.mean((y_pred_test - y_test_mean)**2))\n",
    "        brier_base = float(np.mean((y_train_mean.mean() - y_test_mean)**2))\n",
    "        skill = 1.0 - (brier / max(brier_base, 1e-12))\n",
    "\n",
    "        brier_folds.append(brier)\n",
    "        skill_folds.append(skill)\n",
    "\n",
    "    if len(brier_folds) == 0:\n",
    "        continue\n",
    "\n",
    "    records.append({\n",
    "        \"neuron_idx\": int(nid),\n",
    "        \"Brier_SAE_mean\": float(np.nanmean(brier_folds)),\n",
    "        \"Brier_SAE_std\":  float(np.nanstd(brier_folds)),\n",
    "        \"Skill_mean\":     float(np.nanmean(skill_folds)),\n",
    "        \"Skill_std\":      float(np.nanstd(skill_folds)),\n",
    "        \"K\": K_NEIGHBORS,\n",
    "        \"sae_file\": sae_path.name,\n",
    "        \"enc_key\": enc_key,\n",
    "        \"out_dim\": int(out_dim)\n",
    "    })\n",
    "\n",
    "df_cv = pd.DataFrame(records)\n",
    "out_csv = \"clip_sae_vitl14_knn_filtered_results.csv\"\n",
    "df_cv.to_csv(out_csv, index=False)\n",
    "print(f\"\\n‚úÖ Saved ‚Üí {out_csv}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# PLOT\n",
    "# ---------------------------------------------------------------\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(df_cv[\"Brier_SAE_mean\"], df_cv[\"Skill_mean\"], alpha=0.7)\n",
    "plt.xlabel(\"Brier Loss (test)\")\n",
    "plt.ylabel(\"Skill Score\")\n",
    "plt.title(\"Filtered KNN ‚Äî SAE‚ÜíNeuron mapping\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"clip_sae_vitl14_knn_filtered_plot.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcad7f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuron_idx</th>\n",
       "      <th>Brier_SAE_mean</th>\n",
       "      <th>Brier_SAE_std</th>\n",
       "      <th>Skill_mean</th>\n",
       "      <th>Skill_std</th>\n",
       "      <th>K</th>\n",
       "      <th>sae_file</th>\n",
       "      <th>enc_key</th>\n",
       "      <th>out_dim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3369</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>-0.369999</td>\n",
       "      <td>0.383993</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30339</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>-0.150537</td>\n",
       "      <td>0.167162</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3498</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>-0.154658</td>\n",
       "      <td>0.076424</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25660</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>-0.150348</td>\n",
       "      <td>0.183627</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17205</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.184795</td>\n",
       "      <td>0.245850</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16975</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>-0.219823</td>\n",
       "      <td>0.144573</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27341</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>-0.223031</td>\n",
       "      <td>0.207922</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3692</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>-0.523801</td>\n",
       "      <td>0.575631</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7899</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>-0.063105</td>\n",
       "      <td>0.077034</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33661</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.024112</td>\n",
       "      <td>0.169497</td>\n",
       "      <td>3</td>\n",
       "      <td>model.safetensors</td>\n",
       "      <td>text_model.embeddings.token_embedding.weight</td>\n",
       "      <td>49408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   neuron_idx  Brier_SAE_mean  Brier_SAE_std  Skill_mean  Skill_std  K  \\\n",
       "0        3369        0.003473       0.002412   -0.369999   0.383993  3   \n",
       "1       30339        0.001441       0.000655   -0.150537   0.167162  3   \n",
       "2        3498        0.000917       0.000343   -0.154658   0.076424  3   \n",
       "3       25660        0.000973       0.000286   -0.150348   0.183627  3   \n",
       "4       17205        0.000550       0.000096   -0.184795   0.245850  3   \n",
       "5       16975        0.001197       0.000365   -0.219823   0.144573  3   \n",
       "6       27341        0.007513       0.007234   -0.223031   0.207922  3   \n",
       "7        3692        0.001594       0.001117   -0.523801   0.575631  3   \n",
       "8        7899        0.007062       0.005509   -0.063105   0.077034  3   \n",
       "9       33661        0.002014       0.000764    0.024112   0.169497  3   \n",
       "\n",
       "            sae_file                                       enc_key  out_dim  \n",
       "0  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "1  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "2  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "3  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "4  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "5  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "6  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "7  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "8  model.safetensors  text_model.embeddings.token_embedding.weight    49408  \n",
       "9  model.safetensors  text_model.embeddings.token_embedding.weight    49408  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
