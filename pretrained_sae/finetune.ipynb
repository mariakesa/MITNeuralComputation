{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d37a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--topk TOPK] [--epochs EPOCHS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/run/user/1000/jupyter/runtime/kernel-v39474c2934ae72e6b02ed038579d095698bc6a73b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maria/global_venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ======================================================================\n",
    "# Neural Fine-Tuning of CLIP-SAE-ViT-L-14 (CPU-safe, top-K slicing)\n",
    "# ======================================================================\n",
    "\n",
    "import os, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from safetensors import safe_open\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "import argparse\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Args\n",
    "# ---------------------------------------------------------------\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--topk\", type=int, default=8192,\n",
    "                    help=\"Number of top SAE latents to keep (default: 8192)\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=25)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Config\n",
    "# ---------------------------------------------------------------\n",
    "DEVICE = \"cpu\"      # âœ… safe default\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = \"/home/maria/LuckyMouse/pixel_transformer_neuro/data/processed/hybrid_neural_responses.npy\"\n",
    "IMG_DIR   = Path(\"/home/maria/MITNeuralComputation/vit_embeddings/images\")\n",
    "SAE_REPO  = \"zer0int/CLIP-SAE-ViT-L-14\"\n",
    "LOCAL_SAE_DIR = \"./clip_sae_vitl14_weights\"\n",
    "\n",
    "EPOCHS         = args.epochs\n",
    "LR             = 1e-3\n",
    "BATCH_SIZE     = 4\n",
    "LAMBDA_SPARSE  = 1e-3\n",
    "LAMBDA_NEURAL  = 5e-2\n",
    "VAL_SPLIT      = 0.2\n",
    "TOPK           = args.topk\n",
    "\n",
    "print(f\"ðŸ”¹ Device: {DEVICE} | TOPK={TOPK}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Load neural data\n",
    "# ---------------------------------------------------------------\n",
    "dat = np.load(DATA_PATH)\n",
    "Y_binary = (dat > 0).astype(np.float32)\n",
    "n_neurons, n_samples = Y_binary.shape\n",
    "N_IMAGES = 118\n",
    "N_TRIALS = n_samples // N_IMAGES\n",
    "img_ids_full = np.repeat(np.arange(N_IMAGES), N_TRIALS)\n",
    "Y_image_mean = np.array([Y_binary[:, img_ids_full == i].mean(axis=1)\n",
    "                         for i in range(N_IMAGES)], dtype=np.float32)\n",
    "n_neurons = Y_image_mean.shape[1]\n",
    "print(f\"âœ… Neural data: {n_neurons} neurons Ã— {N_IMAGES} images\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Download SAE weights\n",
    "# ---------------------------------------------------------------\n",
    "snapshot_download(repo_id=SAE_REPO, local_dir=LOCAL_SAE_DIR)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Load CLIP features (precompute if needed)\n",
    "# ---------------------------------------------------------------\n",
    "FEAT_PATH = \"clip_vitl14_feats.npy\"\n",
    "if Path(FEAT_PATH).exists():\n",
    "    X_clip = np.load(FEAT_PATH)\n",
    "else:\n",
    "    print(\"â³ Extracting CLIP features ...\")\n",
    "    clip = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n",
    "    clip.eval(); processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    feats = []\n",
    "    for p in tqdm(sorted(IMG_DIR.glob(\"scene_*.png\"))):\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        inputs = processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            f = clip.get_image_features(**inputs).squeeze().cpu().numpy()\n",
    "        feats.append(f.astype(np.float32))\n",
    "    X_clip = np.stack(feats)\n",
    "    np.save(FEAT_PATH, X_clip)\n",
    "    del clip\n",
    "print(\"âœ… CLIP features:\", X_clip.shape)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Load SAE encoder matrix\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\nðŸ” Scanning for encoder matrices ...\")\n",
    "cands = []\n",
    "for f in Path(LOCAL_SAE_DIR).glob(\"*.safetensors\"):\n",
    "    with safe_open(f, framework=\"pt\", device=\"cpu\") as sf:\n",
    "        for k in sf.keys():\n",
    "            t = sf.get_tensor(k)\n",
    "            if t.ndim == 2:\n",
    "                cands.append((f, k, t.shape))\n",
    "sae_path, enc_key, enc_shape = max(cands, key=lambda x: x[2][0]*x[2][1])\n",
    "print(f\"â†’ Using {sae_path.name} | key={enc_key} | shape={enc_shape}\")\n",
    "\n",
    "with safe_open(sae_path, framework=\"pt\", device=\"cpu\") as sf:\n",
    "    W = sf.get_tensor(enc_key)\n",
    "    b = None\n",
    "    for k in sf.keys():\n",
    "        if \"bias\" in k.lower():\n",
    "            tb = sf.get_tensor(k)\n",
    "            if tb.ndim == 1: b = tb; break\n",
    "\n",
    "A, B = W.shape\n",
    "if A > B: in_dim, out_dim, W_use = B, A, W\n",
    "else:     in_dim, out_dim, W_use = A, B, W.T\n",
    "if (b is None) or (b.shape[0] != out_dim): b = torch.zeros(out_dim)\n",
    "print(f\"âœ“ SAE dims: {in_dim} â†’ {out_dim}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Slice top-K latents\n",
    "# ---------------------------------------------------------------\n",
    "if TOPK < out_dim:\n",
    "    print(f\"ðŸ”¹ Slicing SAE: keeping top {TOPK}/{out_dim} latents\")\n",
    "    W_use = W_use[:TOPK, :]\n",
    "    b = b[:TOPK]\n",
    "    out_dim = TOPK\n",
    "else:\n",
    "    print(f\"ðŸ”¹ Using all {out_dim} latents\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Modules\n",
    "# ---------------------------------------------------------------\n",
    "class SAEEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, W, b):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.copy_(W)\n",
    "            self.linear.bias.copy_(b)\n",
    "    def forward(self, x): return F.relu(self.linear(x))\n",
    "\n",
    "class NeuralFineTuneSAE(nn.Module):\n",
    "    \"\"\"Decoder frozen; only sparsity + neural losses.\"\"\"\n",
    "    def __init__(self, sae: SAEEncoder, n_neurons, Î»s=1e-3, Î»n=5e-2):\n",
    "        super().__init__()\n",
    "        self.encoder = sae.linear\n",
    "        self.neural_head = nn.Linear(sae.linear.out_features, n_neurons, bias=False)\n",
    "        self.Î»s, self.Î»n = Î»s, Î»n\n",
    "    def forward(self, x, y=None):\n",
    "        z = F.relu(self.encoder(x))\n",
    "        loss_s = z.abs().mean()\n",
    "        loss_n = torch.tensor(0.0)\n",
    "        if y is not None:\n",
    "            y_pred = self.neural_head(z)\n",
    "            loss_n = F.mse_loss(y_pred, y)\n",
    "        return self.Î»s*loss_s + self.Î»n*loss_n, z\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Split train/val\n",
    "# ---------------------------------------------------------------\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_clip, Y_image_mean, test_size=VAL_SPLIT, random_state=SEED\n",
    ")\n",
    "X_train_t, X_val_t = torch.tensor(X_train), torch.tensor(X_val)\n",
    "Y_train_t, Y_val_t = torch.tensor(Y_train), torch.tensor(Y_val)\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Training loop\n",
    "# ---------------------------------------------------------------\n",
    "sae = SAEEncoder(in_dim, out_dim, W_use, b)\n",
    "model = NeuralFineTuneSAE(sae, n_neurons, LAMBDA_SPARSE, LAMBDA_NEURAL)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9, nesterov=True)\n",
    "\n",
    "def batches(X, Y, bs):\n",
    "    for i in range(0, len(X), bs):\n",
    "        yield X[i:i+bs], Y[i:i+bs]\n",
    "\n",
    "print(\"\\nðŸš€ Starting training ...\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train(); total = 0.0\n",
    "    for xb, yb in batches(X_train_t, Y_train_t, BATCH_SIZE):\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss, _ = model(xb, yb)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        total += loss.item() * len(xb)\n",
    "    train_loss = total / len(X_train_t)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, _ = model(X_val_t, Y_val_t)\n",
    "    print(f\"Epoch {epoch:02d} | Train {train_loss:.6f} | Val {val_loss.item():.6f}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Evaluation\n",
    "# ---------------------------------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, z_val = model(X_val_t)\n",
    "    Y_pred = model.neural_head(z_val).numpy()\n",
    "    Y_true = Y_val\n",
    "corrs = []\n",
    "for i in range(n_neurons):\n",
    "    xi, yi = Y_pred[:, i], Y_true[:, i]\n",
    "    if np.std(xi) < 1e-8 or np.std(yi) < 1e-8:\n",
    "        corrs.append(np.nan)\n",
    "    else:\n",
    "        corrs.append(pearsonr(xi, yi)[0])\n",
    "corrs = np.array(corrs)\n",
    "mean_corr = np.nanmean(corrs)\n",
    "brier = mean_squared_error(Y_true.flatten(), Y_pred.flatten())\n",
    "\n",
    "print(f\"\\nâœ… Mean neuron corr: {mean_corr:.4f}\")\n",
    "print(f\"âœ… Brier (MSE): {brier:.6f}\")\n",
    "print(f\"âœ… Trainable params: \"\n",
    "      f\"{sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Save\n",
    "# ---------------------------------------------------------------\n",
    "torch.save(model.state_dict(), f\"neural_finetuned_sae_top{TOPK}_cpu.pth\")\n",
    "pd.DataFrame({\"neuron_corr\": corrs}).to_csv(\n",
    "    f\"neural_finetuned_corrs_top{TOPK}_cpu.csv\", index=False)\n",
    "print(f\"\\nðŸ’¾ Saved model â†’ neural_finetuned_sae_top{TOPK}_cpu.pth\")\n",
    "print(f\"ðŸ’¾ Saved correlations â†’ neural_finetuned_corrs_top{TOPK}_cpu.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
