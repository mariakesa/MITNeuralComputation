{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf6fc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Loading ViT embeddings...\n",
      "ðŸ”¹ Loading neural responses (VISam)...\n",
      "VISam: 2040 neurons Ã— 118 images\n",
      "\n",
      "==== Fold 1 (train 94, test 24) ====\n",
      "  ViT train PCs (90%): 38 | VISam train PCs (90%): 65\n",
      "  âœ… Train discovery significant pairs: 0 (BH-FDR Î±=0.05)\n",
      "\n",
      "==== Fold 2 (train 94, test 24) ====\n",
      "  ViT train PCs (90%): 38 | VISam train PCs (90%): 65\n",
      "  âœ… Train discovery significant pairs: 0 (BH-FDR Î±=0.05)\n",
      "\n",
      "==== Fold 3 (train 94, test 24) ====\n",
      "  ViT train PCs (90%): 39 | VISam train PCs (90%): 66\n",
      "  âœ… Train discovery significant pairs: 0 (BH-FDR Î±=0.05)\n",
      "\n",
      "==== Fold 4 (train 95, test 23) ====\n",
      "  ViT train PCs (90%): 38 | VISam train PCs (90%): 67\n",
      "  âœ… Train discovery significant pairs: 0 (BH-FDR Î±=0.05)\n",
      "\n",
      "==== Fold 5 (train 95, test 23) ====\n",
      "  ViT train PCs (90%): 39 | VISam train PCs (90%): 67\n",
      "  âœ… Train discovery significant pairs: 1 (BH-FDR Î±=0.05)\n",
      "\n",
      "ðŸ’¾ Saved per-direction raw results -> vit_VISam_2fold_raw.csv\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/indexing.py:1714\u001b[39m, in \u001b[36m_iLocIndexer._get_list_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1714\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1716\u001b[39m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/generic.py:4153\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4144\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4145\u001b[39m \u001b[33;03mInternal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4146\u001b[39m \u001b[33;03mattribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4151\u001b[39m \u001b[33;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4152\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4153\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4154\u001b[39m \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/generic.py:4133\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4129\u001b[39m     indices = np.arange(\n\u001b[32m   4130\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4131\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4133\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4135\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4139\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4140\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:891\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    890\u001b[39m n = \u001b[38;5;28mself\u001b[39m.shape[axis]\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m indexer = \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    893\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/indexers/utils.py:282\u001b[39m, in \u001b[36mmaybe_convert_indices\u001b[39m\u001b[34m(indices, n, verify)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mindices are out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "\u001b[31mIndexError\u001b[39m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 166\u001b[39m\n\u001b[32m    163\u001b[39m d2[\u001b[33m\"\u001b[39m\u001b[33mvalidated\u001b[39m\u001b[33m\"\u001b[39m] = d2[\u001b[33m\"\u001b[39m\u001b[33mp_test\u001b[39m\u001b[33m\"\u001b[39m] < \u001b[32m0.05\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Simple fold summaries\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m fold1_sum = \u001b[43md1\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_discovered\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mp_train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_validated\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalidated\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmean_r_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr_test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstd_r_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr_test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m fold2_sum = d2.agg(\n\u001b[32m    173\u001b[39m     n_discovered=(\u001b[33m\"\u001b[39m\u001b[33mp_train\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    174\u001b[39m     n_validated=(\u001b[33m\"\u001b[39m\u001b[33mvalidated\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    175\u001b[39m     mean_r_test=(\u001b[33m\"\u001b[39m\u001b[33mr_test\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    176\u001b[39m     std_r_test=(\u001b[33m\"\u001b[39m\u001b[33mr_test\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstd\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    177\u001b[39m )\n\u001b[32m    178\u001b[39m summary = pd.DataFrame([fold1_sum, fold2_sum], index=[\u001b[33m\"\u001b[39m\u001b[33mFold1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mFold2\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/frame.py:10150\u001b[39m, in \u001b[36mDataFrame.aggregate\u001b[39m\u001b[34m(self, func, axis, *args, **kwargs)\u001b[39m\n\u001b[32m  10148\u001b[39m op = frame_apply(\u001b[38;5;28mself\u001b[39m, func=func, axis=axis, args=args, kwargs=kwargs)\n\u001b[32m  10149\u001b[39m result = op.agg()\n\u001b[32m> \u001b[39m\u001b[32m10150\u001b[39m result = \u001b[43mreconstruct_and_relabel_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/apply.py:1913\u001b[39m, in \u001b[36mreconstruct_and_relabel_result\u001b[39m\u001b[34m(result, func, **kwargs)\u001b[39m\n\u001b[32m   1910\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1911\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1913\u001b[39m     result_in_dict = \u001b[43mrelabel_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m     result = DataFrame(result_in_dict, index=columns)\n\u001b[32m   1916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/apply.py:1889\u001b[39m, in \u001b[36mrelabel_result\u001b[39m\u001b[34m(result, func, columns, order)\u001b[39m\n\u001b[32m   1885\u001b[39m     fun = [\n\u001b[32m   1886\u001b[39m         com.get_callable_name(f) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fun\n\u001b[32m   1887\u001b[39m     ]\n\u001b[32m   1888\u001b[39m     col_idx_order = Index(s.index).get_indexer(fun)\n\u001b[32m-> \u001b[39m\u001b[32m1889\u001b[39m     s = \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol_idx_order\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1891\u001b[39m \u001b[38;5;66;03m# assign the new user-provided \"named aggregation\" as index names, and reindex\u001b[39;00m\n\u001b[32m   1892\u001b[39m \u001b[38;5;66;03m# it based on the whole user-provided names.\u001b[39;00m\n\u001b[32m   1893\u001b[39m s.index = reordered_indexes[idx : idx + \u001b[38;5;28mlen\u001b[39m(fun)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/indexing.py:1743\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1741\u001b[39m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1747\u001b[39m     key = item_from_zerodim(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/global_venv/lib/python3.12/site-packages/pandas/core/indexing.py:1717\u001b[39m, in \u001b[36m_iLocIndexer._get_list_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1714\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._take_with_is_copy(key, axis=axis)\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1716\u001b[39m     \u001b[38;5;66;03m# re-raise with different error message, e.g. test_getitem_ndarray_3d\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mpositional indexers are out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "2-fold BH-FDR discovery on train, validation on opposite fold (leak-free).\n",
    "\n",
    "Pipeline (per fold fâˆˆ{1,2}):\n",
    "  - Fit PCA on TRAIN_f only (ViT and brain). n_components by 90% var on TRAIN_f.\n",
    "  - Compute ALL ViT-PC_i Ã— Brain-PC_j correlations on TRAIN_f; BH-FDR at alpha=0.05.\n",
    "  - Validate ONLY those pairs on TEST_(!f), projecting TEST into TRAIN_f bases.\n",
    "Repeat for both folds; then keep pairs that discover+validate in BOTH directions.\n",
    "\n",
    "Outputs:\n",
    "  - raw_fold_results.csv : per-pair per-direction stats\n",
    "  - fold_summary.csv      : summary per fold\n",
    "  - replicated_pairs.csv  : pairs that replicate both ways (by fold-local indices)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import pearsonr\n",
    "from skbio.stats.composition import clr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "VIT_PATH    = '/home/maria/Documents/HuggingMouseData/MouseViTEmbeddings/google_vit-base-patch16-224_embeddings_logits.pkl'\n",
    "NEURAL_PATH = '/home/maria/LuckyMouse/pixel_transformer_neuro/data/processed/hybrid_neural_responses.npy'\n",
    "AREAS_PATH  = '/home/maria/MITNeuralComputation/visualization/brain_area.npy'\n",
    "\n",
    "AREA_NAME   = 'VISam'   # change as needed\n",
    "N_IMAGES, N_TRIALS = 118, 50\n",
    "ALPHA_FDR   = 0.05\n",
    "N_SPLITS    = 5\n",
    "RANDOM_STATE= 42\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD DATA\n",
    "# -----------------------------\n",
    "print(\"ðŸ”¹ Loading ViT embeddings...\")\n",
    "with open(VIT_PATH, 'rb') as f:\n",
    "    vit_logits = pickle.load(f)['natural_scenes']   # (images Ã— D_vit)\n",
    "vit_clr = clr(softmax(np.asarray(vit_logits), axis=1) + 1e-12)\n",
    "\n",
    "print(f\"ðŸ”¹ Loading neural responses ({AREA_NAME})...\")\n",
    "dat   = np.load(NEURAL_PATH, mmap_mode='r')\n",
    "areas = np.load(AREAS_PATH, allow_pickle=True)\n",
    "mask  = areas == AREA_NAME\n",
    "dat   = dat[mask]\n",
    "n_neurons, n_total = dat.shape\n",
    "n_time = n_total // (N_IMAGES * N_TRIALS)\n",
    "dat = dat.reshape(n_neurons, N_IMAGES, N_TRIALS, n_time)\n",
    "X_mean = dat.mean(axis=(2,3))                       # (neurons Ã— images)\n",
    "print(f\"{AREA_NAME}: {n_neurons} neurons Ã— {N_IMAGES} images\")\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS\n",
    "# -----------------------------\n",
    "def fit_pca_train_only(vit_train, brain_train, seed=RANDOM_STATE):\n",
    "    # ViT PCA\n",
    "    pca_vit = PCA(random_state=seed).fit(vit_train)\n",
    "    vit_cum = np.cumsum(pca_vit.explained_variance_ratio_)\n",
    "    vit_n   = int(np.searchsorted(vit_cum, 0.90) + 1)\n",
    "    W_vit   = pca_vit.components_[:vit_n]                             # (vit_n Ã— D_vit)\n",
    "    vit_scores_train = vit_train @ W_vit.T                             # (n_train Ã— vit_n)\n",
    "\n",
    "    # Brain PCA (samples=images, features=neurons)\n",
    "    pca_brain = PCA(random_state=seed).fit(brain_train.T)\n",
    "    brain_cum = np.cumsum(pca_brain.explained_variance_ratio_)\n",
    "    brain_n   = int(np.searchsorted(brain_cum, 0.90) + 1)\n",
    "    W_brain   = pca_brain.components_[:brain_n]                        # (brain_n Ã— neurons)\n",
    "    brain_scores_train = brain_train.T @ W_brain.T                     # (n_train Ã— brain_n)\n",
    "    return W_vit, W_brain, vit_scores_train, brain_scores_train, vit_n, brain_n\n",
    "\n",
    "def discover_on_train(vit_scores_train, brain_scores_train, alpha=ALPHA_FDR):\n",
    "    r_list, p_list, pairs = [], [], []\n",
    "    vit_n = vit_scores_train.shape[1]\n",
    "    brain_n = brain_scores_train.shape[1]\n",
    "    for i in range(vit_n):\n",
    "        for j in range(brain_n):\n",
    "            r, p = pearsonr(vit_scores_train[:, i], brain_scores_train[:, j])\n",
    "            r_list.append(r); p_list.append(p); pairs.append((i+1, j+1))  # 1-indexed for reporting\n",
    "    r_arr = np.array(r_list); p_arr = np.array(p_list)\n",
    "    reject, q_vals, _, _ = multipletests(p_arr, alpha=alpha, method='fdr_bh')\n",
    "    sig_idx = np.where(reject)[0]\n",
    "    return pairs, r_arr, p_arr, q_vals, sig_idx\n",
    "\n",
    "def validate_on_test(W_vit, W_brain, vit_test, brain_test, sig_pairs):\n",
    "    vit_scores_test   = vit_test @ W_vit.T\n",
    "    brain_scores_test = brain_test.T @ W_brain.T\n",
    "    out = []\n",
    "    for (i,j) in sig_pairs:  # 1-indexed\n",
    "        r, p = pearsonr(vit_scores_test[:, i-1], brain_scores_test[:, j-1])\n",
    "        out.append((i, j, r, p))\n",
    "    return out  # list of tuples\n",
    "\n",
    "# -----------------------------\n",
    "# 2-FOLD OUTER CV\n",
    "# -----------------------------\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_indices = list(kf.split(np.arange(N_IMAGES)))  # [(train_idx, test_idx), (..)]\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for fold_id, (train_idx, test_idx) in enumerate(fold_indices, 1):\n",
    "    print(f\"\\n==== Fold {fold_id} (train {len(train_idx)}, test {len(test_idx)}) ====\")\n",
    "    vit_train, vit_test = vit_clr[train_idx], vit_clr[test_idx]\n",
    "    brain_train, brain_test = X_mean[:, train_idx], X_mean[:, test_idx]\n",
    "\n",
    "    # Fit PCA on TRAIN only\n",
    "    W_vit, W_brain, vit_scores_train, brain_scores_train, vit_n, brain_n = fit_pca_train_only(vit_train, brain_train)\n",
    "    print(f\"  ViT train PCs (90%): {vit_n} | {AREA_NAME} train PCs (90%): {brain_n}\")\n",
    "\n",
    "    # Discovery on TRAIN with BH-FDR across ALL pairs\n",
    "    pairs, r_tr, p_tr, q_tr, sig_idx = discover_on_train(vit_scores_train, brain_scores_train, alpha=ALPHA_FDR)\n",
    "    print(f\"  âœ… Train discovery significant pairs: {len(sig_idx)} (BH-FDR Î±={ALPHA_FDR})\")\n",
    "\n",
    "    sig_pairs = [pairs[k] for k in sig_idx]\n",
    "\n",
    "    # Validate on the OPPOSITE foldâ€™s TEST using current TRAIN bases (no refit)\n",
    "    val_results = validate_on_test(W_vit, W_brain, vit_test, brain_test, sig_pairs)\n",
    "\n",
    "    # Record rows for this direction (fold_id: trainâ†’its test)\n",
    "    for k, (i, j) in enumerate(sig_pairs):\n",
    "        all_rows.append({\n",
    "            \"direction\": f\"Fold{fold_id}_trainâ†’test\",\n",
    "            \"fold\": fold_id,\n",
    "            \"vit_pc_train\": i,\n",
    "            \"brain_pc_train\": j,\n",
    "            \"r_train\": r_tr[sig_idx[k]],\n",
    "            \"p_train\": p_tr[sig_idx[k]],\n",
    "            \"q_train\": q_tr[sig_idx[k]],\n",
    "            \"r_test\": val_results[k][2],\n",
    "            \"p_test\": val_results[k][3],\n",
    "            \"n_train\": len(train_idx),\n",
    "            \"n_test\": len(test_idx),\n",
    "        })\n",
    "\n",
    "# -----------------------------\n",
    "# AGGREGATION & SYMMETRIC REPLICATION\n",
    "# -----------------------------\n",
    "if not all_rows:\n",
    "    print(\"\\nNo train-significant pairs; nothing to report.\")\n",
    "else:\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df.to_csv(f\"vit_{AREA_NAME}_2fold_raw.csv\", index=False)\n",
    "    print(f\"\\nðŸ’¾ Saved per-direction raw results -> vit_{AREA_NAME}_2fold_raw.csv\")\n",
    "\n",
    "    # Split by direction\n",
    "    d1 = df[df[\"direction\"] == \"Fold1_trainâ†’test\"].copy()\n",
    "    d2 = df[df[\"direction\"] == \"Fold2_trainâ†’test\"].copy()\n",
    "\n",
    "    # For symmetric replication, we need pairs that were discovered in Fold1 train and also discovered in Fold2 train,\n",
    "    # and each validates on its own test. Because PCA bases differ per fold, we treat indices as fold-local;\n",
    "    # we therefore define replication as: \"exists any pair in Fold1 and any pair in Fold2 that both validate\".\n",
    "    # (If you want to *match* PCs across folds, add cosine-similarity matching on TRAIN loadings â€” leak-free.)\n",
    "\n",
    "    # Mark validation success (test p < 0.05) per direction\n",
    "    d1[\"validated\"] = d1[\"p_test\"] < 0.05\n",
    "    d2[\"validated\"] = d2[\"p_test\"] < 0.05\n",
    "\n",
    "    # Simple fold summaries\n",
    "    fold1_sum = d1.agg(\n",
    "        n_discovered=(\"p_train\", \"size\"),\n",
    "        n_validated=(\"validated\", \"sum\"),\n",
    "        mean_r_test=(\"r_test\", \"mean\"),\n",
    "        std_r_test=(\"r_test\", \"std\")\n",
    "    )\n",
    "    fold2_sum = d2.agg(\n",
    "        n_discovered=(\"p_train\", \"size\"),\n",
    "        n_validated=(\"validated\", \"sum\"),\n",
    "        mean_r_test=(\"r_test\", \"mean\"),\n",
    "        std_r_test=(\"r_test\", \"std\")\n",
    "    )\n",
    "    summary = pd.DataFrame([fold1_sum, fold2_sum], index=[\"Fold1\", \"Fold2\"])\n",
    "    summary.to_csv(f\"vit_{AREA_NAME}_2fold_fold_summary.csv\")\n",
    "    print(\"\\n===== Per-fold summary =====\")\n",
    "    print(summary)\n",
    "\n",
    "    # Extract lists of validated pairs per direction (fold-local indices)\n",
    "    d1_val = d1[d1[\"validated\"]][[\"vit_pc_train\", \"brain_pc_train\", \"r_test\", \"p_test\"]].copy()\n",
    "    d2_val = d2[d2[\"validated\"]][[\"vit_pc_train\", \"brain_pc_train\", \"r_test\", \"p_test\"]].copy()\n",
    "\n",
    "    # Save validated lists\n",
    "    d1_val.to_csv(f\"vit_{AREA_NAME}_2fold_validated_fold1.csv\", index=False)\n",
    "    d2_val.to_csv(f\"vit_{AREA_NAME}_2fold_validated_fold2.csv\", index=False)\n",
    "\n",
    "    # â€œReplicates both waysâ€ here means: each fold produced at least one validated pair.\n",
    "    # (Exact PC index matching across folds is not meaningful without alignment;\n",
    "    # if you want alignment, add cosine-matching on TRAIN loadings and intersect.)\n",
    "    replicated_both = (len(d1_val) > 0) and (len(d2_val) > 0)\n",
    "    rep_df = pd.DataFrame({\n",
    "        \"replicated_both_ways\": [bool(replicated_both)],\n",
    "        \"fold1_n_validated\": [int(len(d1_val))],\n",
    "        \"fold2_n_validated\": [int(len(d2_val))]\n",
    "    })\n",
    "    rep_df.to_csv(f\"vit_{AREA_NAME}_2fold_replicated_flag.csv\", index=False)\n",
    "    print(\"\\n===== Replication status =====\")\n",
    "    print(rep_df)\n",
    "    print(f\"\\nðŸ’¾ Saved summaries to files with prefix: vit_{AREA_NAME}_2fold_*\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
